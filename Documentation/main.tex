\documentclass[12pt]{article}
\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst} 
\usepackage{hyperref}
\usepackage{geometry}
\usepackage[dvipsnames]{xcolor}
\usepackage[nottoc]{tocbibind}
\usepackage[linesnumbered]{algorithm2e}
\usepackage{textcomp}
\usepackage{listings,chngcntr}
\usepackage{float}
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage{tikz}
\usepackage{pgfplots}

\usetikzlibrary{shapes,positioning,intersections,quotes}

\newtheorem{defin}{Definicja}
\newtheorem{sample}{Przykład}
\renewcommand*{\algorithmcfname}{Algorytm}
 
\newcommand{\sectionbreak}{\clearpage}
 \geometry{
 a4paper,
 left=35mm,
 top=25mm,
 bottom=25mm,
 right=25mm,
}

\definecolor{lbcolor}{gray}{0.96}
\lstset{
  backgroundcolor=\color{lbcolor},
  tabsize=4,    
  %   rulecolor=,
  language=[GNU]C++,
  basicstyle=\scriptsize,
  upquote=true,
  aboveskip={1.5\baselineskip},
  columns=fixed,
  showstringspaces=false,
  extendedchars=false,
  breaklines=true,
  prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
  frame=single,
  numbers=left,
  showtabs=false,
  showspaces=false,
  showstringspaces=false,
  identifierstyle=\ttfamily,
  keywordstyle=\color[rgb]{0,0,1},
  commentstyle=\color[rgb]{0.026,0.112,0.095},
  stringstyle=\color[rgb]{0.627,0.126,0.941},
  numberstyle=\color[rgb]{0.205, 0.142, 0.73}
}
\lstset{
  backgroundcolor=\color{lbcolor},
  tabsize=4,
  language=C++,
  captionpos=b,
  frame=lines,
  numbers=left,
  numberstyle=\tiny,
  numbersep=5pt,
  breaklines=true,
  showstringspaces=false,
  basicstyle=\footnotesize,
  % identifierstyle=\color{magenta},
  keywordstyle=\color[rgb]{0,0,1},
  commentstyle=\color{Darkgreen},
  stringstyle=\color{red}
  keywordstyle = [2]{\color{Violet}},
  keywordstyle = [3]{\color{WildStrawberry}},
  keywordstyle = [4]{\color{Mulberry}},
  otherkeywords = {::,<<,>>,>,<,=,++,--},
  morekeywords = [2]{::},
  morekeywords = [3]{<<, >>, >, <, =},
  morekeywords = [4]{++,--},
}
\def\inline{\lstinline[basicstyle=\ttfamily,keywordstyle={}]}

\begin{document}
\counterwithin{lstlisting}{section}
\begin{titlepage}
	\centering
	{\scshape\LARGE Politechnika Poznańska \par}
	{\scshape\LARGE Wydział Informatyki \par}
	{\scshape\LARGE Instytut Informatyki \par}
	\vspace{1cm}
	{\scshape\Large Praca dyplomowa inżynierska\par}
	\vspace{1.5cm}
	{\huge\bfseries Implementacja algorytmu eksploracji danych z użyciem CUDA API\par}
	\vspace{2cm}
	{\Large\itshape Marcin Jabłoński \par}
	{\Large\itshape Łukasz Kosiak \par}
	{\Large\itshape Piotr Kurzawa \par}
	{\Large\itshape Marek Rydlewski \par}
	\vfill
	\begin{flushright}
	Promotor:\par
	dr inż. ~Witold \textsc{Andrzejewski}
	\end{flushright}
	\vfill
	{\large Poznań, 2017 r.\par}
\end{titlepage}
\thispagestyle{empty} % Strona z pustym stylem, bez numeru
$\mbox{ }$
\vfill\vfill
\hfill
\begin{flushright}
\begin{em}
,,Coś się popsuło`` \\
Zbigniew Stonoga
\end{em}
\end{flushright}
\vfill\pagebreak
\tableofcontents
\newpage

\section{Wstęp}

\subsection{Wprowadzenie}
Informatyzacja życia codziennego, jaka dokonała się w ostatnich latach sprawiła, że każdego dnia konsumenci często nieświadomie zostawiają po sobie wiele informacji na swój temat. Nawet z pozoru niewinne dane o ludzkich przyzwyczajeniach typu "z~której półki bierzemy bułki w sklepie" są zapisywane w systemach informatycznych. Należy do tego oczywiście dodać inne usługi, które wybierane są przez użytkowników świadomie np. zapisywanie lokalizacji przez prywatny telefon komórkowy.

Wbrew pozorom, taka błaha na pierwszy rzut oka informacja może mieć jednak istotne znaczenie dla funkcjonowania przemysłu piekarskiego. Nic nie stoi na przeszkodzie, aby spróbować z tych danych odczytać preferencje bądź przyzwyczajenia przeciętnego Kowalskiego na temat jego codziennych zakupów, które mogą w przyszłości zaprocentować - zarówno dla właściciela, jak i klienta. Jest to oczywiście tylko przykład, ale oddaje doskonale fakt przydatności z pozoru nie mających znaczenia prostych czynności człowieka, jakie często przypadkiem rejestrują działające wokół konsumentów systemy.

Pozostaje jednak problem przetworzenia takich danych w celu otrzymania interesującej informacji, która byłaby potencjalnie użyteczna. Trzeba pamiętać, że rozmiar takich danych nierzadko sięga terabajtów i w praktyce skuteczna analiza takich danych przez człowieka nie jest możliwa. Musi on zatem w tym celu skorzystać z~dobrodziejstw, jakie przynosi mu współczesna technologia.

Problem efektywnego przetwarzania zdążył urosnąć do rangi oddzielnego działu w informatyce. W pracy \cite{kdd} zasugerowano utworzenie nowej dyscypliny mającej na celu opracowanie technik obliczeniowych rozwiązujących takie problemy, zwanej odkrywaniem wiedzy w bazach danych (ang. KDD – \textit{Knowledge Discovery in Databases}). Techniki te mają na celu odnajdywanie prawidłowych, nietrywialnych i~potencjalnie użytecznych wzorców w dużych zbiorach danych.

Wspominane wyżej techniki w dużej mierze zależą od rodzaju bazy, a ściślej mówiąc - charakteru danych w niej występujących. W przypadku danych zawierających informację o położeniu zazwyczaj mowa jest o  odkrywaniu wiedzy w bazach danych przestrzennych (ang. \textit{spatial data mining}). Takie systemy mogą zawierać atrybut lokalizacji obiektu w danym obszarze, jego opis w formie geometrycznej (np. w postaci wielokątów), a także inne atrybuty nieprzestrzenne. Okazuje się, że tradycyjne metody analizy danych przestrzennych zazwyczaj nie radzą sobie z nimi na tyle efektywnie, by było opłacalne ich użycie w praktyce \cite{trad}, dlatego też zaczęto szukać nowych sposobów na odkrywanie wiedzy w takich bazach.

W pracy \cite{huang} zaproponowano odkrywanie \textit{wzorców kolokacji przestrzennych} (lub krócej: \textit{kolokacji}), czyli zbioru cech przestrzennych występujących w niewielkiej odległości od siebie.  Łatwo to można sobie wyobrazić na przykładzie przyrody, gdzie osobniki (gatunki) o podobnych cechach zazwyczaj trzymają się razem. Rozumowanie to działa również dla bliższych współczesnemu człowiekowi cech przestrzennych, np. punktach o podobnej funkcji - stacje, kina, piekarnie, itd. Wraz z rosnącą popularnością obliczeń na kartach graficznych (w dużej mierze spowodowana wprowadzeniem technologii \textit{CUDA} autorstwa firmy NVIDIA) pojawiło się wiele gotowych rozwiązań, pozwalających na efektywne wyszukiwanie kolokacji nawet w bardzo rozbudowanych bazach danych. Przegląd niektórych z nich można znaleźć w pracy \cite{boinski}.

Ostatni rok przyniósł kolejną metodę efektywnego przeszukiwania baz danych w~celu odnalezienia kolokacji \cite{chinczyki}. Wykorzystuje ona autorski algorytm wyszukiwania maksymalnych klik w~grafie rzadkim oraz skondensowane drzewa instancji przechowywaczce kliki instancji dla każdego kandydata do kolokacji (patrz Rozdział~\ref{sec:china}) w~celu zmniejszenia czasu obliczeń oraz ograniczenia wymagań co do pamięci operacyjnej. Algorytm ten jest przedmiotem badań niniejszej pracy zbiorowej.

\subsection{Cel i zakres pracy}

Celem niniejszej pracy jest analiza wydajności zaproponowanych w pracy \cite{chinczyki} rozwiązań z zakresu odkrywania kolokacji przestrzennych dla GPU i CPU.

Zakres pracy obejmuje następujące zadania szczegółowe:

\begin{enumerate}
\item \textbf{Zapoznanie się z literaturą.} Zapoznanie się z podstawowymi pojęciami dotyczącymi odkrywania danych w bazach danych przestrzennych oraz wyszukiwania wzorców kolokacji przestrzennych jest niezbędne do stworzenia działającej implementacji powyższego algorytmu. Dodatkowo należy zwrócić uwagę na dodatkowe zagadnienia związane z teorią grafów.
\item \textbf{Opracowanie wersji równoległej algorytmu eksploracji danych.} Konieczne jest przemyślenie wykorzystania algorytmów pomocniczych dla poszczególnych kroków całego rozwiązania oraz zaproponowanie możliwie najkorzystniejszego rozwiązania biorąc pod uwagę dostępną pamięć operacyjną, czas przetwarzania i przesyłania danych między pamięcią operacyjną a pamięcią karty graficznej.
\item \textbf{Implementacja wersji sekwencyjnej i równoległej ww. algorytmu.} Rozwiązanie podane w punkcie drugim powinno zostać zaimplementowane w~technologii NVIDIA CUDA dla wersji GPU oraz biblioteki PPL w przypadku odmiany dla CPU.
\item \textbf{Przeprowadzenie eksperymentów wydajnościowych.} Analiza wyników testów wydajnościowych implementacji z punktu 3 jest głównym celem tej pracy. Należy zbadać efektywność obu rozwiązań pod względem czasu wykonywania oraz zapotrzebowania na dostępną pamięć. 
\end{enumerate}

\subsection{Charakterystyka źródeł}

Jak już wspomniano, niniejsza praca w dużej mierze opiera się o algorytm zaprezentowany w dokumencie \cite{chinczyki}. Do jej opracowania była wymagana wiedza zawarta w~innych źródłach, często również o charakterze naukowym.

Głównym źródłem wiedzy na temat kolokacji przestrzennych była rozprawda doktorska dr inż. Pawła Boińskiego \cite{boinski}, która w dużym przekroju omawia ideę kolokacji zaprezentowaną przez Shekhara i Huanga w pracy \cite{huang}, a także prezentuje najpopularniejsze techniki ich odkrywania (metody \textit{Co-location Miner}, \textit{iCPI-tree}). Część rozwiązań wykorzystanych w tych technikach została wykorzystana w trakcie realizacji algorytmu.

Oddzielną kwestią jest literatura książkowa, wykorzystana do zapoznania się z~technologią CUDA oraz przyjęcia dobrych praktyk optymalizacyjnych i programistycznych. Tutaj szczególnie należy wymienić popularną pozycję \textit{CUDA w przykładach} autorstwa Shane'a Cooke'a \cite{cuda_by_examples}, a także \textit{Professional CUDA C Programming} \cite{professional_cuda} będącą również podstawą do wstępu teoretycznego w Rozdziale~\ref{sec:cuda}.

\subsection{Struktura pracy}

\textbf{Rozdział~\ref{sec:theory}} stanowi wstęp teoretyczny do niniejszej pracy - prezentuje podstawowe pojęcia związane z odkrywaniem kolokacji przestrzennych, a także prezentuje najprostsze algorytmy wyszukujące wzorce kolokacji przestrzennych.

\textbf{Rozdział~\ref{sec:cuda}} zawiera wprowadzenie do technologii CUDA. Przedstawia także podstawowe różnice w programowaniu na CPU i GPU oraz wyjaśnia podstawowe pojęcia związane z programowaniem na procesory graficzne.

\textbf{Rozdział~\ref{sec:china}} poświęcony jest algorytmowi \textit{SGCT} będącym głównym tematem pracy. Zawiera wyjaśnienia poszczególnych kroków algorytmu oraz niezbędne definicje.

Implementacja CPU (w wersji sekwencyjnej, jak i równoległej) jest głównym tematem \textbf{Rozdziału~\ref{sec:cpu}}. Koncentruje się on w szczególności na przedstawienie zastosowanych technik optymalizacyjnych poprawiających wydajność algorytmu w środowisku CPU. Rozdział zawiera także krótki wstęp do biblioteki \textit{PPL} użytej w~równoległej implementacji algorytmu.

\textbf{Rozdział~\ref{sec:gpu}} zawiera omówienie implementacji algorytmu dla procesorów graficznych (GPU). Z uwagi na specyfikę środowiska programistycznego \textit{CUDA} opis skupia się na sposobach uruchamiania poszczególnych kroków algorytmów za pomocą grup wątków.

Szczegóły dotyczące przeprowadzonych testów oraz porównanie wydajności zaimplementowanych rozwiązań, wraz z komentarzem i wykresami, zostało zawarte w~\textbf{Rozdziale~\ref{sec:tests}}. 

Ostatni \textbf{Rozdział~\ref{sec:fin}} stanowi podsumowanie całego projektu, uwagi zespołu oraz możliwe dalsze kierunki rozwoju.

\subsection{Podział pracy}

Podział zadań w ramach niniejszego projektu wyglądał następująco:

\begin{itemize}
\item \textbf{Marcin Jabłoński} - równoległa implementacja CPU, testy;
\item \textbf{Łukasz Kosiak} - implementacja GPU, testy;
\item \textbf{Piotr Kurzawa} - tekst pracy inżynierskiej, testy;
\item \textbf{Marek Rydlewski} - sekwencyjna i równoległa implementacja CPU, testy.
\end{itemize}

\newpage

\section{Podstawy teoretyczne}
\label{sec:theory}

\subsection{Charakterystyka danych przestrzennych}

\subsubsection{Modelowanie danych przestrzennych}

Sposób reprezentacji danych przestrzennej w dużej mierze zależy od zastosowań, niemniej najczęściej przybiera jedną z następujących form:

\begin{itemize}
\item \textit{model pól} - ma formę funkcji, której dziedzina należy do modelowanej przestrzeni, a jego wynikiem jest cecha przestrzenna;
\item \textit{model obiektowy} - dla każdego zjawiska jest tworzony nowy obiekt z odpowiednimi właściwościami (etykietami, atrybutami przestrzennymi i nieprzestrzennymi).
\end{itemize}

W praktyce model pól używany jest przede wszystkim w metodach opartych na dokonywaniu pomiarów z powietrza - takie dane mają wtedy charakter rastrowy (reprezentacja w postaci pikseli). Model obiektowy stosowany jest natomiast w przypadkach, gdzie występuje duża liczba dodatkowych atrybutów nieprzestrzennych.

\subsubsection{Źródła danych przestrzennych}

Najogólniej źródła danych przestrzennych można podzielić ze względu na ich format.

\textit{Pierwotne źródła danych} są opracowane w jednym ze standardowych formatów źródeł (najczęściej dla konkretnego systemu) i nie wymagają jakichkolwiek transformacji. Mają one zazwyczaj postać cyfrową i pochodzą z automatycznych pomiarów dokonanych przez specjalizowane systemy wyposażone w odbiorniki GPS czy tachimetry.

\textit{Wtórne dane źródłowe} nie zostały zebrane z myślą o wykorzystaniu w \textit{systemach informacji geograficznej} (ang. \textit{Geographic Information System}, GIS) i dlatego wymagają one odpowiedniej transformacji oraz cyfryzacji (jeżeli są one analogowe). Procedury te są one obarczone pewnym ryzykiem, ponieważ istnieje możliwość wystąpienia błędów w trakcie konwersji i w konsekwencji przekłamaniami w danych wynikowych, które należy ręcznie poprawić.

\subsubsection{Relacje}

Określenie zachodzących relacji między obiektami w źródłach danych przestrzennych jest ważnym elementem przetwarzania danych przestrzennych. Sposób ich określenia zależy od zastosowanego modelu danych.

W modelu pól relacje determinowane są przez operacje pól (ang. \textit{field operations}, \cite{fieldmodel}), mogące przybierać różne formy w zależności od zastosowań, natomiast w modelu obiektowym rodzaje relacji przestrzennych zależą od definicji przestrzeni. Według standardu organizacji OGC (\textit{Open Geospatial Consortium}) istnieją trzy najpopularniejsze rodzaje związków przestrzennych między obiektami:

\begin{itemize}
\item \textit{Relacje metryczne} - wyrażane w postaci predykatów typu "w odległości nie większej niż 10 metrów", oparte na odległości;
\item \textit{Relacje kierunkowe} - położenie określone jest względem globalnych kierunków dla przestrzeni (np. na północ, na południe - są to relacje bezwzględne) lub względem innego obiektu/obserwatora (nazywamy takie relacjami względnymi);
\item \textit{Relacje topologiczne} - najbardziej skomplikowane, wyrażone przez zależności typu pokrywanie, zawieranie, styczność.
\end{itemize}

W systemach typu GIS stosuje się głównie relacje topologiczne. Mają one postać predykatów przestrzennych dla operacji filtrowania i połączenia przestrzennego w~językach zapytań działających na danych przestrzennych. Najczęściej wykorzystuje się je w tzw. \textit{modelu dziewięciu przecięć} \cite{9sec}, za pomocą którego określa się możliwe relacje zachodzącą dla pary obiektów.

Dla każdego obiektu wyznacza się jego wnętrze, granicę i zewnętrze. Następnie, dokonuje się operacji przecięcia dla danej pary obiektów dla każdej z możliwych kombinacji elementów tego obiektu (np. granica pierwszego obiektu z wnętrzem drugiego). Takich relacji w dwuwymiarowej relacji można wyznaczyć osiem, należą do nich np. rozłączność, styczność, częściowe i całkowite pokrycie itd. 

Istnieje również rozszerzenie modelu dziewięciu przecięć, zwanym DE-9IM (ang. \textit{Dimensionally Extended nine-Intersection Model}, \cite{9sec2}), które rozróżnia rodzaj obiektu uzyskanego w wyniku przecięcia (mogą być puste, bezwymiarowe, jednowymiarowe i dwuwymiarowe). 

\subsection{Metody eksploracji danych przestrzennych}

Specyfika danych przestrzennych, a w szczególności fakt, że własności obiektu w danych przestrzennych mogą zależeć od cech jego sąsiadów, powoduje, że stosowanie klasycznych metod eksploracji danych może doprowadzić do nieprawidłowych wyników \cite{klasykchuj1} \cite{klasykchuj2} - stąd też istnieje konieczność korzystania z metod eksploracji dedykowanych dla danych przestrzennych. Wiele z nich jest tak naprawdę rozwinięciem metod opracowanych dla klasycznych zbiorów danych.

\subsubsection{Grupowanie przestrzenne}

Metoda grupowania przestrzennego (ang. \textit{spatial clustering}) zakłada istnienie przestrzeni $m$-wymiarowej, w której znajdują się punkty odpowiadające obiektom. Przestrzeń ta ma rozkład niejednorodny, a każdy z obiektów jest opisany przez $m$~atrybutów. Celem grupowania jest poszukiwanie gęstych obszarów punktów używając w tym celu metryki Euklidesowej jako funkcji podobieństwa (bliskości). 

Grupowanie przestrzenne w największym stopniu spośród wszystkich metod eksploracji danych przestrzennych jest podobna do swojego klasycznego odpowiednika - wiele algorytmów grupowania opracowanych dla klasycznych zbiorów danych zadziała również dla danych przestrzennych. 

\subsubsection{Klasyfikacja przestrzenna}

Klasyfikacja przestrzenna (ang. \textit{spatial classification}) działa podobnie jak jego odmiana dla danych klasycznych - przewiduje klasy nowych obiektów w oparciu o~tzw. zbiór uczący, składający się ze wcześniejszych obserwacji.

W celu dostosowania klasyfikacji dla danych przestrzennych zaproponowano \cite{klasyfikacja} wykorzystanie \textit{grafu sąsiedztwa} będącego reprezentacją relacji przestrzennych między obiektami, w którym wierzchołki stanowią obiekty przestrzenne, a relacje są krawędziami. Następnie w takim grafie wyznaczane są wszystkie ścieżki, których początkiem jest analizowany obiekt. Dalej analiza przebiega zgodnie z algorytmem \textit{ID3} \cite{id3}.

Z pojęciem klasyfikacji przestrzennej wiąże się także predykcja położenia (ang. \textit{location prediction}), czyli przewidywanie zdarzeń we wskazanym miejscu w przestrzeni, przy uwzględnieniu autokorelacji przestrzennej. Przydaje się ono np. w określaniu regionów o wysokim ryzyku wystąpienia klęsk żywiołowych czy awarii.

\subsubsection{Odkrywanie trendów przestrzennych}

Trend przestrzenny definuje się \cite{toptrendy} jako regularną zmianę co najmniej jednego atrybutu nieprzestrzennego obiektów wraz z oddalaniem się od innego obiektu. Odkrywanie trendów sprowadza się zazwyczaj do analizy regresji, gdzie odległość od danego obiektu jest zmienną niezależną, natomiast różnica wartości atrybutów do obserwacji - zmienną zależną.

Trendy dzieli się na globalne i lokalne. Pierwsze wskazują na zwiększanie (bądź zmniejszanie) wartości obserwowanych atrybutów przy rozpatrywaniu wszystkich obiektów znajdujących się na ścieżkach wychodzących z punktu początkowego. Typowym przykładem jest wzrost bezrobocia wraz z oddalaniem się od centrum miast. Trend lokalny jest reprezentowany przez pojedyncze ścieżki wykazujące inny kierunek zmian na danym atrybucie niż na sąsiednich ścieżkach.

\subsubsection{Przypadki osobliwe w danych przestrzennych}

Czasem w danych przestrzennych można znaleźć obiekty, których atrybuty nieprzestrzenne są niespójne z innymi obserwacjami dokonanymi w ich otoczeniu. Noszą one miano \textit{przypadków osobliwych} \cite{przypadeg}. 

Wyszukiwanie takich zjawisk jest trudne, szczególnie gdy istnieje więcej atrybutów nieprzestrzennych - odwzorowanie ich w $n$-wymiarowej przestrzeni może skutkować \textit{przekleństwem wielowymiarowości} (ang. \textit{curse of dimensionality}) \cite{kurwa}, utrudnionym rozróżnianiem obiektów podobnych do siebie.

\subsubsection{Asocjacje przestrzenne}

Problem odkrywania asocjacji został pierwszy raz zdefiniowany w pracy \cite{asoc} i~w~ogólności polega na analizie dostępnych transakcji (zbiorów obiektów, np. koszyka zakupów) oraz wykryciu występujących w nich regularności występowania elementów (typu: klient, który kupując bułki wybrał także masło).

Najczęściej reguły charakteryzuje się miarą \textit{wsparcia} (ang. \textit{support}) i \textit{ufności} (ang. \textit{confidence}). Pierwsza z nich wyraża stosunek występowania transakcji zawierającą lewą i prawą stronę reguły do ilości wszystkich transakcji. Ufność z kolei wskazuje na procentowy udział transakcji zawierających lewą i prawą stronę reguły we wszystkich transakcjach, które zawierają jego lewą stronę (jest to tzw. prawdopodobieństwo warunkowe). W celu ograniczenia ilości wykrytych wzorców wprowadzono także pojęcie \textit{zbioru częstego} (ang. \textit{frequent itemsets}) - zbioru elementów, dla których wyznaczone wsparcie przekracza pewien ustalony przez użytkownika próg minimalnego wsparcia. 

Z pracy \cite{asoc} pochodzi także popularny algorytm wykorzystywany w wielu metodach odkrywania asocjacji i kolokacji, czyli metoda \textit{Apriori}. Wykorzystuje on ważną cechę miary wsparcia, jaką jest \textit{antymonotoniczność}. Wynika z niej, że zbiór może być zbiorem częstym tylko w przypadku, kiedy jego podzbiory są również zbiorami częstymi. 

Na początku algorytmu generowane są jednoelementowe zbiory częste. Następnie iteracyjnie wykonywane są następujące kroki:
\begin{itemize}
\item tworzenie zbiorów częstych $ (i + 1) $-elementowych na podstawie zbiorów o długości $ i $,
\item filtrowanie zbiorów kandydujących w oparciu o miarę wsparcia,
\item dodanie kandydatów do zbioru wynikowego.
\end{itemize}

Generowanie kandydatów polega na łączeniu wszystkich par zbiorów częstych o identycznych elementach początkowych, a następnie usuwaniu tych, które nie są zbiorami częstymi w oparciu o własność antymonotoniczności. Algorytm kończy się, gdy zbiór kandydatów będzie pusty.  

W celu dostosowania metody odkrywania asocjacji do danych przestrzennych wprowadzono pojęcie \textit{przestrzennej reguły asocjacyjnej} \cite{asoc2}. Zakłada ona istnienie predykatów przestrzennych (mogących wyrażać informacje o odległości czy kierunku), które mogą występować zarówno w części warunkującej (poprzedniku), jak i~w~warunkowanej (następniku). Następnie podczas procesu odkrywania przestrzennych reguł asocjacyjnych dane umieszczone w ciągłej przestrzeni są zamieniane na zbiór transakcji. Metoda ta jest zaliczana do modelu zorientowanego na cechę referencyjną \cite{boinski}. Istnieje też inne podejście, zwane \textit{odkrywaniem zbiorów częstych klas sąsiadów}, opisane w pracy \cite{classsets}. 

\subsubsection{Kolokacje przestrzenne}

Przedstawiony w pracy \cite{huang} problem \textit{odkrywania przestrzennych reguł kolokacyjnych} powstał w odpowiedzi na niedoskonałości asocjacji (w szczególności konieczność wyboru cechy referencyjnej) i zakłada istnienie równorzędnych cech przestrzennych. 

Praca wprowadza pojęcie \textit{wzorca kolokacji przestrzennej} (zwanego także kolokacją przestrzenną lub krócej - kolokacją), zbioru cech przestrzennych, których instancje często występują we wzajemnym sąsiedztwie \cite{boinski}. Stanowi on swego rodzaju odpowiednik zbiorów częstych w asocjacjach przestrzennych. Również miara wsparcia została zastąpiona przez \textit{miarę powszechności} (ang. prevalence), które eliminują wymaganie wiedzy o transakcjach. 

Kolokacje przestrzenne są przykładem modelu zorientowanego na zdarzenie (ang. \textit{event-centric model}).

\subsection{Odkrywanie kolokacji przestrzennych}

Niniejszy rozdział zawiera opisy i definicje pojęć niezbędnych do zrozumienia algorytmu zawartego w rozdziale 3.

\subsubsection{Cecha przestrzenna}

Kluczową kwestią w procesie odkrywania kolokacji jest odpowiednia klasyfikacja obiektów występujących w bazie danych. Każdy zbiór danych przestrzennych, oprócz informacji o lokalizacji obiektu i opisujących go danych nieprzestrzennych powinien zawierać także właściwość pozwalającą na sklasyfikowanie danego obiektu do określonej klasy. Takie przypisanie nazywane jest cechą przestrzenną (ang. spatial feature) lub rzadziej klasą obiektu (ang. object class).

Jako typowy przykład cechy przestrzennej można podać etykietę przypisaną do obiektu na mapie (np. kościół, szkoła, strzelnica). Pozwala ona na jednoznaczne określenie własności przestrzeni w punkcie, gdzie znajduje się obiekt.

\subsubsection{Podstawowe definicje}

\begin{defin}[Relacja sąsiedztwa]
Dany jest graf $ G \in (E, V) $. Relacja sąsiedztwa R to relacja istniejąca pomiędzy wierzchołkami w grafie, taka, że jeżeli para (u, v) stanowi krawędź grafu G, to wierzchołek v jest sąsiedni do wierzchołka u. 
\end{defin}

\begin{defin}[Instancja cechy przestrzennej]
Niech f będzie cechą przestrzenną. Mówimy, że obiekt x jest instancją cechy przestrzennej f, wtedy i tylko wtedy, gdy obiekt x jest typu f oraz jest opisany przez lokalizację i identyfikator.
\end{defin}

\begin{defin}[Wzorzec i instancja kolokacji]
Załóżmy $F$ jako zbiór cech przestrzennych $F = \{ f_{1}, f_{2}, ...,f_{m} \} $ , a $FI = FI^{f_{1}} \cup FI^{f_{2}} \cup ... \cup FI^{f_{m}}$ niech będzie
zbiorem ich instancji. Niech $ >_{F} $ oznacza dowolną relację porządku zdefiniowaną dla zbioru $ F $. Niech $ f_{i} $ oznacza i-tą cechę przestrzenną (ze względu na relację $ >_{F} $), zatem $ \forall i,j \in 1,...,m $ $ f_{i} <_{F} $ $ f_{j} \Leftrightarrow i < j \land f_{i},f_{j} \in F $. Mając daną relację sąsiedztwa R (zwrotną i przechodnią) mówimy, że wzorzec kolokacji przestrzennej (w skrócie "kolokacja") jest podzbiorem cech przestrzennych $ c \subseteq F $ , których instancje $ I\subseteq FI $ tworzą klikę ze względu na relację R. Zbiór wszystkich instancji kolokacji przestrzennej $c$ jest oznaczany przez $CI^{c} $. Przez długość kolokacji należy rozumieć liczbę elementów w zbiorze cech przestrzennych, który tworzy tę kolokację.
\end{defin}

\begin{defin}[Sąsiedztwo]
Mające daną zwrotną i symetryczną relację sąsiedztwa R, sąsiedztwem lokalizacji l nazywamy zbiór lokalizacji $L = \{l_{1},l_{2}, . . . , l_{n}\}$, gdzie $l_{i}$ jest sąsiadem l, tzn. zachodzi $R(l, l_{i}) $ $ \forall i \in 1,...,n$.
\end{defin}

\begin{figure}
\centering
\begin{tikzpicture}[scale=2.5, transform shape]
\draw[lightgray] (0,-0) grid[xstep=0.5, ystep=0.5]  (3,3);
\draw[draw=black!30,latex-] (0,3) +(0,0.5cm) node[above right] {$y$} -- (0,0) -- +(0, 0cm);
\draw[draw=black!30,-latex] (0,0) +(0cm,0) -- (3,0) -- +(0.5cm,0) node[below right] {$x$};
\draw [gray, dashed] (1.0,2) circle (0.9cm);
\draw [gray, dashed] (1.3,1.4) circle (0.9cm);

\node [fill, draw, circle, minimum width=3pt, inner sep=0pt, pin={[outer sep=-2pt]335: $ l_1 $ }] at (.3,.7) {};
\node [fill, draw, circle, minimum width=3pt, inner sep=0pt, pin={[outer sep=-2pt]25: $ l_2 $}] at (1.0,2) {};
\node [fill, draw, circle, minimum width=3pt, inner sep=0pt, pin={[outer sep=-2pt]155:$ l_3 $}] at (1.3,1.4) {};
\node [fill, draw, circle, minimum width=3pt, inner sep=0pt, pin={[outer sep=-2pt]155:$ l_4 $}] at (2.7,0.5) {};
\end{tikzpicture}
\caption{Relacja sąsiedztwa - lokalizacje \textit{l}\textsubscript{2} oraz \textit{l}\textsubscript{3} sąsiadują ze sobą.}
\end{figure}

\subsubsection{Miary kolokacji}

\begin{defin}[Współczynnik uczestnictwa]
Współczynnik uczestnictwa (ang. participation ratio) cechy $ f_{i} $ w kolokacji c jest równy procentowemu udziałowi wszystkich instancji cechy $ f_{i} $ w instancjach kolokacji c:

\begin{equation}
pr(f_{i}, c) = \frac{|\pi^{f_{i}}(CI^{c})|}{FI^{f_{i}}}
\end{equation}
gdzie $ \pi^{f_{i}}(CI^{c})$ oznacza projekcję relacyjną zbioru instancji $ CI^{c}$ względem cechy $f_{i}$ (z usuwaniem duplikatów).
\end{defin}

\begin{defin}[Indeks uczestnictwa]Indeks uczestnictwa (ang. participation index) kolokacji c jest równy najmniejszemu ze współczynników uczestnictwa wyznaczonych dla każdej cechy przestrzennej $ f_{i} \in c$:
\begin{equation}
pi(c) = min_{f_{i} \in c} pr(f_{i} ,c)
\end{equation}
Indeks uczestnictwa najczęściej określany jest w literaturze mianem miary powszechności lub krótko powszechnością kolokacji.
\label{def:prevalence}
\end{defin}

\begin{defin}[Maksymalny wzorzec kolokacji przestrzennej]Niech będzie dana wartość min\_prev oznaczająca pewien minimalny próg powszechności. Jeżeli $ c = \{f_{1},...,f_{m} \} $ jest kolokacją powszechną (tzn. $ pi(c) \ge min\_prev $) i nie istnieje żaden nadzbiór c taki, że powszechność dla tego nadzbioru jest równa co najmniej min\_prev, kolokacja c nazywana jest kolokacją maksymalną.  
\end{defin}

\subsubsection{Problem}

%\begin{defin}[Reguła kolokacyjna]Reguła kolokacyjna to reguła postaci $ c_{1} \rightarrow c_{2}(p, cp)$, gdzie $ c_{1} \subseteq F $, $c_{2} \subseteq F $ i $c _{1} \cup c_{2} = \emptyset $. Potencjalna użyteczność reguły może być mierzona przy pomocy jej powszechności p oraz prawdopodobieństwa warunkowego cp.
%\end{defin}

\begin{defin}[Prawdopodobieństwo warunkowe]Prawdopodobieństwem warunkowym
$ cp(c_{1}, c_{2})$ reguły kolokacyjnej $ c_{1} \rightarrow c_{2} $ nazywamy stosunek liczby instancji wzorca c 1 w sąsiedztwie instancji wzorca $ c_{2}$ do liczby wszystkich instancji wzorca $ c_{1} $:
\begin{equation}
cp(c_{1}, c_{2}) = \frac{|\pi^{c_{1}}(CI^{c_{1} \cup c_{2}})|}{CI^{c_{1}}}
\end{equation}
gdzie $\pi^{c_{1}}(CI^{c_{1} \cup c_{2}})$ oznacza projekcję relacyjną instancji wzorca $CI^{c_{1} \cup c_{2}}$ względem wzorca $ c_{1} $ (z usuwaniem duplikatów).
\end{defin}

\begin{defin}[Problem odkrywania kolokacji]
Problem odkrywania kolokacji przestrzennych jest zdefiniowany w następujący sposób.
Mając dane:
\begin{itemize}
\item zbiór cech przestrzennych $F = \{ f_{1}, f_{2}, ...,f_{m} \} $
\item zbiór obiektów $FI = FI^{f_{1}} \cup FI^{f_{2}} \cup ... \cup FI^{f_{m}}$  , gdzie $ FI^{f_{i}},(0 < i \le m) $ jest zbiorem instancji cechy $ f_{i}$, przy czym każda instancja jest opisana przez lokalizację i identyfikator,
\item symetryczną i zwrotną relację sąsiedztwa $R$,
\item próg minimalnej powszechności min\_prev oraz próg minimalnego prawdopodobieństwa warunkowego min\_cond,
\end{itemize}
znajdź wszystkie poprawne reguły kolokacyjne z powszechnością nie mniejszą niż min\_prev i prawdopodobieństwem warunkowym nie mniejszym niż min\_cond.
\end{defin}

\subsection{Przegląd algorytmów odkrywania wzorców kolokacji przestrzennych}

W tym podrozdziale zostaną zaprezentowane skrótowo najważniejsze algorytmy odkrywania kolokacji przestrzennych. 

\subsubsection{Co-location Miner}
\label{subsec:clm}

Wraz z wprowadzeniem pojęcia kolokacji autorzy pracy \cite{huang} zaprezentowali także podstawowy obecnie algorytm rozwiązujący problem odkrywania wzorców kolokacji przestrzennych, zwany \textit{Co-location Miner}. W algorytmie tym wyróżnia się następujące fazy:

\begin{itemize}
\item generowanie kandydatów na kolokacje przestrzenne (o długości $i$),
\item wyznaczanie instancji dla wygenerowanych kandydatów,
\item usuwanie kandydatów, których powszechność wynosi mniej niż przyjęty próg minimalnej powszechności.
\end{itemize}

Pozostali kandydaci trafiają do zbioru wynikowego, a następnie na ich podstawie są tworzone reguły kolokacyjne. Same reguły również podlegają filtracji - usuwane są te reguły, których prawdopodobieństwo warunkowe jest poniżej określonego progu. 

W następnej iteracji algorytm wykonuje dokładnie te same kroki, przy czym generowani kandydaci są o długości o jeden większej. Całość kończy się, gdy nie jest możliwe już wygenerowanie nowych kandydatów.

\subsubsection{Multiresolution Co-location Miner}

Korzystanie z oryginalnego algorytmu \textit{Co-location Miner} wiąże się niestety z~dużymi kosztami obliczeniowymi, głównie ze względu na pracochłonny krok generowania kandydatów na kolokacje. Dlatego też niedługo później w pracy \cite{multihuang} autorzy zaproponowali drobną modyfikację oryginalnego algorytmu, dodając dodatkowy krok filtrowania w opraciu o przybliżoną reprezentację zbioru wejściowego.

W algorytmie \textit{Multiresolution Co-location Miner} zbiór wejściowy zostaje podzielony na obszary (mniejsze fragmenty). Zanim rozpocznie się faza wyznaczania instancji dla wygenerowanych kandydatów, nastepuje szacowanie ich powszechności na podstawie sąsiadujących instancji cech przestrzennych w ramach obszarów. W przypadku zbyt niskiej wartości szacowanej powszechności kandydata, można go wykluczyć z dalszego przetwarzania i tym samym oszczędzić zasoby niezbędne na wyznaczenie jego instancji.

Dalsze kroki przebiegają identycznie jak w przypadku oryginalnego \textit{Co-location Miner}.

\subsubsection{Joinless}

Celem autorów pracy \cite{joinless} było stworzenie algorytmu, który omijałby konieczność tworzenia kosztownych połączeń przestrzennych na etapie wyznaczania instancji kandydatów na kolokacje (tak jak np. w przypadku rodziny algorytmów \textit{Co-location Miner}). Nosi on nazwę algorytmu bezpołączeniowego (ang. \textit{joinless}).

Główną różnicą w porównaniu do wcześniejszych algorytmów jest sposób generowania instancji kolokacji. Są one generowane na podstawie sąsiedztw typu gwiazda - zbiorów obiektów, w którego skład wchodzi rozpatrywany obiekt oraz jego sąsiedzi posiadający większą cechę przestrzenną. Wyznacza się je na podstawie oddzielnych algorytmów (np. \textit{plane sweep}), lub korzysta z specjalnych struktur ułatwiających wykrywanie sąsiadów typu \textit{R-drzewo}.

Wygenerowane instancje muszą zostać dodatkowo zweryfikowane (poprawne instancje powinny być kliką, czego nie gwarantuje sąsiedztwo typu gwiazda), a następnie - podobnie jak w algorytmie \textit{Multiresolution Co-location Miner} - dokonuje się ich wstępnego filtrowania pod kątem progu minimalnej powszechności.

\subsubsection{iCPI-tree}

Drzewo iCPI (\textit{improved Co-location Pattern Instance}, \cite{icpi}) stanowi zmodyfikowaną odmianę drzewa CPI zawartego w pracy \cite{cpi}. Struktura ta zawiera informacje o wszystkich zachodzących relacjach sąsiedztwa. 

\textit{iCPI-tree} posiada następującą strukturę:

\begin{itemize}
\item Poziom 1 - korzeń drzewa (oznaczony etykietą \textit{NULL}),
\item Poziom 2 - cechy elementów centralnych, czyli cechy przestrzenne obiektów centralnych \textit{sąsiedztw typu gwiazda};
\item Poziom 3 - instancje elementów centralnych, dla których ma zostać przechowana informacja o sąsiadach;
\item Poziom 4 - cechy sąsiadów,
\item Poziom 5 - instancje sąsiadów.
\end{itemize}

Sąsiedzi uporządkowani są według rosnącej cechy przestrzennej, a w przypadku instancji tej samej cechy - zgodnie z rosnącym identyfikatorem. Takie uporządkowanie nosi nazwę \textit{uporządkowanego zbioru sąsiadów}.

Powyższa struktura drzewiasta jest wykorzystana w algorytmie w celu generowania instancji coraz dłuższych kandydatów w kolejnych iteracjach. Dokonuje się tego poprzez systematyczną ich rozbudowę o kolejne elementy. Na początku wszystkie instancje są jednoelementowe, a w kolejnych iteracjach są one rozbudowywane poprzez wyszukiwanie sąsiadów z odpowiednią cechą i weryfikowane (należy sprawdzić, czy nowo dodany obiekt do instancji jest sąsiadem każdego z obiektów należących do tej instancji). 

Pozostałe kroki algorytmu (generowanie kandydatów i reguł, filtrowanie według powszechności) są podobne jak w metodach \textit{Co-location Miner} i \textit{joinless}.

\newpage

\section{Wprowadzenie do technologii CUDA}
\label{sec:cuda}

Dziedzina informatyki, jaką są obliczenia ogólnego przeznaczenia na układach GPU (ang. \textit{general-purpose computing on graphics processing units}, w skrócie \textit{GPGPU}) należy do stosunkowo świeżych technik programowania - jej właściwy początek można datować na okolice 2007 roku. Jeszcze do niedawna wśród programistów panowało przekonanie, że karty graficzne powinny być odpowiedzialne tylko za rysowanie obrazu, a cała odpowiedzialność za niezbędne obliczenia powinna padać na CPU, jako "serce" komputera. Były to czasy, kiedy pojęcie równoległego wykonywania zadań nie było szeroko rozpowszechnione - rdzenie procesora były jedynie nowinką, a cały rozwój procesorów CPU szedł w zwiększanie częstotliwości taktowania.

Szybko się okazało, że nie da się tego robić w nieskończoność. Podnoszenie częstotliwości spowodowało, że procesory zaczęły pobierać spore ilości energii, a także wydzielać ogromne ilości ciepła, którego nie dało się okiełznać bez korzystania z~specjalnego chłodzenia. Rozwiązaniem okazało się skorzystanie z wielordzeniowości - wielu procesorów połączonych ze sobą specjalnymi magistralami, zamkniętymi w~jednej obudowie. Wymagało to także zmiany podejścia do programowania. Równoległość daje olbrzymie możliwości, o ile potrafi się z nich skorzystać w odpowiedni sposób - bez tego programista nie uzyska zauważalnego wzrostu szybkości obliczeń, jakie dają współczesne układy wielordzeniowe. 

Wielu osobom w trakcie tej "równoległej rewolucji" umknął pewien fakt - kiedy swoje triumfy świecił procesor \textit{Pentium 4} firmy \textit{Intel} o częstotliwości taktowania rzędu 3 GHz, na rynku istniały już rozwiązania równoległe, na których można było uzyskać znacznie lepsze wyniki. Były to oczywiście karty graficzne, które od dawna działały w sposób równoległy. 

Dopóki nie powstały pierwsze specyfikacje bibliotek wspierających obliczenia na kartach graficznych, programiści próbowali wykorzystywać potencjał drzemiący w~procesorach graficznych w oparciu o rendering 3D. Było to dosyć karkołomne zadanie i nie każdy algorytm można było rozwiązać w ten sposób. Karty graficzne musiały odczekać jeszcze parę lat, aby dało się wykorzystać w pełni ich możliwości obliczeniowe.

Spośród wszystkich technologii umożliwiających przeprowadzanie obliczeń na kartach graficznych zdecydowanie największą popularność osiągnęła pierwsza z nich, czyli CUDA (ang. \textit{Compute Unified Device Architecture}). Została ona zaprezentowana po raz pierwszy w 2007 roku przez firmę NVIDIA i jest dedykowana wyłącznie produktom tej firmy (w porównaniu do konkurencyjnej technologii OpenCL).

\subsection{CPU a GPU}

Jak już wspomiano, procesory graficzne, mające w pierwotnym założeniu wyłącznie wspomagać generowanie obrazu, od początku były projektowane jako układy przetwarzajace dane w sposób równoległy. Procesory zawarte w kartach graficznych posiadają znacznie więcej rdzeni niż CPU - mają one jednak mniejsze taktowanie, a także są gorzej wyposażone (w szczególności brakuje im rozszerzeń typowych dla CPU, typu \textit{SSE2} czy \textit{MMX}). W związku z tym, nie wszystkie algorytmy nadają się dobrze do implementacji w środowisku GPU. Jeżeli jednak istnieje możliwość zrównoleglenia jakiegoś problemu zgodnie z paradygmatem SIMD, to wyniki osiągnięte na GPU będą w większości wypadków lepsze niż uzyskane na procesorze CPU. Obecne procesory CPU z kolei potrafią działać sekwencyjnie, jak i równolegle, za pośrednictwem wielu mechanizmów wspierających równoległość (np. wątki, mutexy, wsparcie dla specjalnych struktur).

Oddzielną kwestią jest przydział tranzystorów tworzących poszczególne układy do pełnienia poszczególnych funkcji. W przypadku CPU większość tranzystorów stanowi pamięć podręczną oraz układy sterowania - za wykonywanie obliczeń jest odpowiedzialna ok. 1/4 tranzystorów wbudowanych w procesor. Natomiast w przypadku GPU gros tranzystorów wykorzystanych jest do budowy jednostek arytmetyczno-logicznych oraz zmiennoprzecinkowych. Z tego powodu duża część zadań związana z przepływem danych, uruchamianiem wątków czy podziałem zadań na GPU spoczywa na programiście, co dosyć mocno komplikuje kod.

\subsection{Architektura CUDA}

Procesor graficzny kompatybilny z CUDA zbudowany jest z wielu multiprocesorów strumieniowych (ang. \textit{streaming multiprocessors}, w skrócie SM). Na każdy z~nich składa się określona ilość rdzeni CUDA, które mogą przetwarzać dane w sposób równoległy - w każdym z nim jest umieszczona dodatkowo jednostka arytmetyczno-logiczna (ALU, umożliwia obliczenia na liczbach całkowitych) oraz jednostka zmiennoprzecinkowa (FPU). Do tego w każdym \textit{SM} znajdują się jednostki funkcji specjalnych - w nich wyznaczane są wartości funkcji matematycznych typu pierwiastkowanie czy funkcje trygonometryczne. Liczba powyższych elementów nie jest stała - zależą one od miary \textit{potencjału obliczeniowego CUDA} (ang. \textit{CUDA Compute Capability}), który obsługuje dany sprzęt.

Każdy \textit{SM} zawiera 16 jednostek wejścia i wyjścia, mających na celu pośredniczenie w przekazywaniu danych między różnymi typami pamięci. Przez nie odbywa się transfer danych do pamięci RAM karty graficznej (zlokalizowanej poza procesorem graficznym). W multiprocesorze są dostępne następujące typy pamięci podręcznej:

\begin{itemize}
\item pamięć rejestrów - najszybsza pamięć, w której wątek może przechowywać swoje zmienne,
\item pamięć wspólna (ang. \textit{shared memory}) - pamięć, poprzez którą może odbywać się wymiana danych między wątkami;
\item pamięć podręczna pierwszego poziomu (ang. \textit{L1 cache}).
\end{itemize}

Oprócz tego multiprocesor wyposażony jest w układy sterujące wywołaniem instrukcji (ang. \textit{Instruction Dispatch Unit}) odpowiadające za wykonywanie obliczeń na zgrupowanych rdzeniach.

\subsection{Programowanie wielowątkowe w języku CUDA C}

Środowisko programistyczne \textit{CUDA Toolkit} umożliwia programowanie w języku \textit{CUDA C} - stanowi on podzbiór języka C++ i jest z nim w dużym stopniu kompatybilny. Kod w języku \textit{CUDA C} da się łączyć z standardowym kodem C++ kompilowanym pod procesory CPU - w tym przypadku fragmenty kodu wywoływane po stronie karty graficznej są kompilowane przez specjalny kompilator \textit{NVCC} dołączany do środowiska CUDA. 

Funkcje wywoływane przez hosta, a wykonywane przez procesor graficzny w~sposób równoległy przez wiele wątków są nazywane funkcjami jądra (ang. kernel). Wszystkie wątki uruchomione poprzez wywołanie jądra wykonują identyczny kod. Oczywiście da się je odróżnić poprzez unikalny identyfikator dostępny w specjalnej zmiennej. Zgodnie z modelem przetwarzania SIMD, na jednym multiprocesorze uruchomione wątki są grupowane w tzw. \textit{warpy} (ang. \textit{warps}) - zbiory 32 wątków wykonujących tą samą instrukcję. 

Wątki można organizować w większe grupy. Podstawową grupą wątków jest \textit{blok}, w ramach którego można uformować wątki w jednym, dwóch, a nawet trzech wymiarach. Bloki grupowane są z kolei w siatkę obliczeniową (ang. \textit{computation grid}), która również może składać się z trzech wymiarów. Przy wywołaniu kernela programista określa liczbę równoległych bloków oraz ilość wątków na blok, za pomocą których urządzenie będzie przetwarzać równolegle kernel. Odpowiednie ustawienie tych wartości jest kluczowe dla wydajności przetwarzania - zbyt mała/wielka ilość przyznanych bloków może spowodować odwrotny skutek do zamierzonego. Maksymalne ilości wątków na blok i bloków na siatkę określa miara \textit{potencjału obliczeniowego CUDA}. 

\subsection{Model pamięci}

Karty graficzne obsługujące technologię CUDA posiadają następujące typy pamięci, dostępne dla programisty:

\begin{itemize}
\item \textit{rejestry} - zdecydowanie najszybsza i najwygodniejsza pamięć, umieszczona w~procesorze graficznym, mała, ograniczona żywotność do czasu życia wątku;
\item \textit{pamięć lokalna} - umiejscowiona w pamięci karty graficznej, wolna, będąca alternatywą do rejestrów;
\item \textit{pamięć wspólna} - dostęp do niej możliwy jest ze wszystkich wątków pracujących w ramach jednego bloku, mała (umiejscowiona w chipie GPU), lecz o~szybkim czasie dostępu;
\item \textit{pamięć globalna} - duża (rzędu paru gigabajtów), wolna, o dużym czasie dostępu, dostęp z poziomu każdego wątku oraz hosta;
\item \textit{pamięć stała} - dostępna dla wszystkich wątków w trybie tylko do odczytu, buforowana, posiada unikalną możliwość rozgłaszania danych (ang. \textit{broadcast});
\item \textit{pamięć tekstur} - również tylko do odczytu, przeznaczona z myślą o przechowywaniu danych w postaci macierzy.
\end{itemize}

\subsection{\textit{Thrust}}

Biblioteka \textit{Thrust} stanowi odpowiednik popularnej \textit{Standard Template Library} dla języka CUDA C. Od wersji 1.4.0 jego wersja produkcyjna jest dołączana wraz z~środowiskiem \textit{CUDA Toolkit 4.0}.

Podobnie jak jego odpowiednik dla języka C++, Thrust zawiera bogatą kolekcję wbudowanych algorytmów i operacji dostosowanych do równoległego środowiska CUDA. W szczególności dostarcza równoległe operacje skanowania, sortowania czy redukcji, zwalniając programistę od implementowania własnych, potencjalnie niewydajnych rozwiązań.

Thrust zawiera struktury znane z biblioteki STL, takie jak wektory (w wersji dla hosta oraz karty graficznej), iteratory oraz wiele innych typów generycznych ułatwiających korzystanie z możliwości dostarczanych przez środowisko CUDA C. Użycie ich jest analogiczne jak w "tradycyjnym" STL-u.

Oprócz tego, biblioteka \textit{Thrust} oferuje następujące operacje:

\begin{itemize}
\item \textit{Inclusive scan} – wyznaczenie tablicy w oparciu o tablicę o rozmiarze k, takiej, że $ B[i] = \sum\limits_{j=0}^{i} A[j], i \in (0, k-1) $;
\item \textit{Exclusive scan} – wyznaczenie tablicy w oparciu o tablicę o rozmiarze k, takiej, że $ B[0] = 0 \cap B[i] = \sum\limits_{j=0}^{i-1} A[j], i \in (1,k-1) $;
\item \textit{Compact} – usuwanie pozycji z tablicy wejściowej spełniającej podany warunek, często na podstawie innej tablicy;
\item \textit{Unique} – wyznaczenie unikalnych wartości z (posortowanej) tablicy wejściowej,
\item \textit{Sort} – sortowanie tablicy podanej na wejściu, również na podstawie klucza sortowania będącego oddzielną tablicą.
\end{itemize}
\newpage

\section{Algorytm \textit{SGCT}}
\label{sec:china}

Niniejszy rozdział ma za zadanie przybliżenie algorytmu będącego tematem tej pracy - metody odkrywania maksymalnych kolokacji przestrzennych w oparciu o graf rzadki i skondensowane drzewo instancji (ang. \textit{sparse-graph and condensed tree-based maximal co-location algorithm}) przedstawionej w pracy \cite{chinczyki}. 

Poszczególne kroki pierwotnego algorytmu \textit{SGCT} zostaną opisane w kolejnych podrozdziałach. Szczegóły implementacji wraz z zaproponowanymi usprawieniami znajdują się w Rozdziale 4.

\subsection{Generowanie tabeli instancji kolokacji o rozmiarze 2}

Pierwszy krok algorytmu jest podobny do metody \textit{Co-location Miner} i polega na wygenerowaniu 2-elementowych kandydatów na kolokacje. 

Kolokacje o rozmiarze 2 tworzone są na podstawie wygenerowanych w oparciu o~cechy przestrzenne jednoelementowych kolokacji. Nie jest do tego wykorzystywana jednak metoda \textit{Apriori}, ponieważ udowodniono w pracy \cite{huang}, że dla kandydatów dwuelementowych lepszą wydajność można uzyskać w oparciu o algorytm \textit{spatial join}. Wykorzytany został zatem algorytm \textit{sweeping-based spatial join} \cite{spatial} z dodatkową modyfikacją, usuwającą pary instancji o tej samej cesze przestrzennej. 

\begin{sample}Przykładowe zapytanie tworzące kandydatów na kolokacje o rozmiarze 2 przestawia się następująco:

\textbf{select} $ p', p''$

\textbf{from} $ \{p_{1},...,p_{12}\} p' $, $ \{p_{1},...,p_{12}\} p''$

\textbf{where}  $ p' $.feature $ \neq $ $ p'' $.feature, $ p' \lq p'' $, $(p', p'') \in R$
\end{sample}

Na podstawie wygenerowanych kolokacji oraz tzw. \textit{progu odległości} (ang. \textit{distance threshold}) tworzona jest dwuwymiarowa tablica z haszowaniem (ang. \textit{hash table}). Jest ona indeksowana cechami przestrzennymi. Każdy element tablicy zawiera wskaźnik do listy zawierającej instancje kandydatów o odpowiadających indeksom cechach przestrzennych. Instancja zostanie dodana do tej listy tylko wtedy, gdy odległość między instancjami nie przekracza dopuszczalnego progu odległości między nimi.

%\begin{sample}
%Zapytanie $InsTable_{2}(A,B)$ zwróci listę kandydatów $(A_{2}, B_{2})$, $(A_{3}, B_{1})$, itd. Nie znajdziemy na tej liście pary $(A_{2}, B_{100000})$, gdyż odległość między tymi instancjami przekracza dopuszczalny próg odległości.
%\end{sample}

\subsection{Obliczanie miary powszechności}

Również krok obliczania powszechności dla kandydatów nie różni się od tego znanego z \textit{Co-location Miner}.

Dla każdego kandydata w tabeli wyliczany jest współczynnik uczestnictwa (ang. \textit{participation index}). Dokonuje się tego poprzez wybieranie wszystkich unikalnych instancji cech przestrzennych, która są ujęte w danej kolokacji. Następnie zgodnie z~definicją miary powszechności z tabeli instancji są usuwani kandydaci, dla których obliczona miara powszechności jest mniejsza niż zadany próg minimalnej powszechności $ min\_prev $ \cite{huang}. 

\subsection{Generowanie kandydatów na kolokacje maksymalne}

Krok ten wprowadza nową strukturę, zwaną \textit{grafem kolokacji o rozmiarze 2} (ang. \textit{size-2 co-location graph}). Jego definicja brzmi następująco:

\begin{defin}[Graf kolokacji o rozmiarze 2]
Jeżeli przyjąć relacje sąsiedztwa między kolokacjami o rozmiarze 2 jako krawędzie $ E = \{e_{1},...,e_{u}\}$, a cechy przestrzenne występujące w kolokacjach jako wierzchołki $ V = \{v_{1},...,v_{\lambda}\}$, gdzie $ u $ i $ \lambda $ są odpowiednio liczbą krawędzi i liczbą wierzchołków, to graf kolokacji o rozmiarze 2 można zamodelować jako graf nieskierowany $ G= (V, E)$, przechowywany w listowej strukturze danych uporządkowanej rosnąco. Zbiór N jest zbiorem sąsiedztw wierzchołka i definiuje się go następująco:
\begin{equation}
N(v_{i}) = \{W|{v_{i},w} \in E\}
\end{equation}
\label{def:size2-col-graph}
\end{defin}

Zadaniem tego kroku jest wyszukanie w takim grafie maksymalnych klik, określanych jako \textit{kandydaci na maksymalne kolokacje} (ang. \textit{candidate maximal co-location}).

\begin{defin}[Kandydat na maksymalną kolokację]
Kandydat na maksymalną kolokację $ C_{m} $ składa się z uporządkowanych cech przestrzennych o następujących właściwościach: każda para cech w  $ C_{m} $  jest ze sobą połączona krawędzią, a żadne dodatkowe cechy nie mogą być dodane do $ C_{m} $ bez zachowania ich kompletnego połączenia.
\end{defin}

Autorzy pracy \cite{chinczyki} udowodnili, że graf kolokacji o rozmiarze 2 można traktować jako graf rzadki. Umożliwia to efektywne korzystanie z algorytmu Brona-Kerboscha \cite{kerbosz} do wyszukiwania maksymalnych klik w grafie nieskierowanym. Wprowadzone zostały do niego pewne modyfikacje uwzględniające rozproszenie grafu oraz wybieranie \textit{pivotu} w celu usprawienia wyszukiwania kandydatów na kolokacje (patrz Algorytm~\ref{alg:chinczyki_step3}).

\begin{algorithm}

\SetKwInOut{Input}{Wejście}
\SetKwInOut{Output}{Wyjście}
\SetKwProg{myproc}{Procedure}{}{}
\SetKwFunction{proc}{BK\_Pivot}
\Input{$G=(E,V)$}
\Output{$CP_{m}$}
$CP_{m} \leftarrow \emptyset; X \leftarrow \emptyset; P \leftarrow \emptyset$\;
\ForEach{$v^{*}_{i}$ in lista $v^{*}_{1}, v^{*}_{2},..., v^{*}_{\lambda}$  uporządkowanych wg. degeneracji}{
$ P \leftarrow N(V^{*}_{i}) \cup \{ v^{*}_{i+1}, ..., v^{*}_{\lambda}\}$\;
$ X \leftarrow N(V^{*}_{i}) \cup \{ v^{*}_{1}, ..., v^{*}_{i-1}\}$\;
$BK\_Pivot(P,  \{v^{*}_{i}\} ,X)$\;
}

\myproc{\proc{M,K,T}}{
\lIf{$ M \cup T = \emptyset$}{$ \{ CP_{m} \leftarrow CP_{m} \cup K\}$}
wybór $ pivotu $ $ u \in M \cup T ; \% $ do maksymalizacji $ | M \cap N(u) | $\;
\ForEach{$v_{i} \in M \backslash N(u) $}{
$BK\_Pivot(M \cap N(V_{i}), K \cup \{v_{i}\}, T \cap N(V_{i}))$\;
$ M \leftarrow M \backslash \{v_{i}\} $\;
$ T \leftarrow T \cup \{v_{i}\}$\;
}
}
\caption{Generowanie maksymalnych kandydatów na kolokacje}
\label{alg:chinczyki_step3}
\end{algorithm}

Pierwsza z modyfikacji oryginalnego algorytmu dodaje mechanizm \textit{pivoting selection} opisany pierwotnie w pracy \cite{pivot} (linia 9, zobacz także Algorytm~\ref{alg:tomita_tanaka}). Jak wykazano w pracy \cite{pivot2} wybranie wierzchołka zmniejszającego liczbę rekurencyjnych wywołań algortytmu znacząco zmniejsza ogólny czas wykonania. Wybiera on wierzchołek będący osią podziału zbioru (tzw. \textit{pivot}) w oparciu o rozmiar unii sąsiadów tego wierzchołka i wierzchołków kandydatów. Każda maksymalna klika musi zawierać albo wierzchołek $ u $, albo niesąsiadujące z nim wierzchołki - jeżeli nie zawiera, zostanie on dodany (linie 11-13). W związku z tym, tylko wierzchołek $ u $ i jego nie-sąsiedzi muszą być przetestowani (linia 10).

\begin{algorithm}

\SetKwInOut{Input}{Wejście}
\SetKwInOut{Output}{Wyjście}
\SetKwProg{myproc}{Procedure}{}{}
\SetKwFunction{proc}{TomitaMaximalPivot}
\Input{$G=(E,V)$}
\Output{$ u $}
\myproc{\proc{SUBG, CAND}}{
$ MaxCard \leftarrow 0; u \leftarrow \emptyset $\;
\ForEach{$ s \in SUBG $}{
$ N \leftarrow $ uporządkowani sąsiedzi $ s $\;
$ CAND' \leftarrow CAND \cup N $\;
\If{$|CAND'| \geq MaxCard $}{
$ u \leftarrow s $\;
$ MaxCard \leftarrow |CAND'| $\;
}
}
}
\caption{Algorytm wybierania osi podziału metodą Tomity i Tanaki (2006)}
\label{alg:tomita_tanaka}
\end{algorithm}

Druga modyfikacja opiera się o pojęcie rozproszenia grafu. Opisuje się je miarą \textit{degeneracji grafu} \cite{matusiak}:
\begin{defin}[Degeneracja grafu]
Degeneracja grafu G jest najmniejszą wartością k, taką, że każdy niepusty podgraf G zawiera wierzchołki o stopniu co najwyżej k. Oznacza to, że wielkość maksymalnej kliki nie może przekroczyć $k + 1$.
\label{def:degeneracy}
\end{defin}

\begin{defin}[Uporządkowanie wg. miary degeneracji]
Uporządkowanie według miary degeneracji wierzchołków grafu G to takie uporządkowanie, które minimalizuje stopień degeneracji grafu. Taka uporządkowanie gwarantuje m.in. optymalną kolejność kolorowania w problemie kolorowania wierzchołków.
\end{defin}

Wierzchołki w zewnętrznej rekurencji są uporządkowane według stopnia degeneracji (linia 2). W ciele rekurencji (linie 3-5) liczba wierzchołków czekających na weryfikację nie przekroczy $ k $. Tym sposobem ograniczono liczbę zewnętrznych rekurencji. Dla grafów o małym stopniu degeneracji obserwuje się duży wzrost wydajności \cite{degenerat}.

Autorzy pracy \cite{wang} udowodnili, że zbiór wynikowy kandydatów na kolokacje zawsze będzie zawierał wszystkie maksymalne kliki spełniające próg powszechności. Jest to jeden z warunków poprawności algorytmu generowania maksymalnych kandydatów na kolokacje.

\subsection{Proces odcinania}

W ostatnim kroku w oparciu o tzw. \textit{prunning framework} \cite{wang} nastepuje otrzymywanie końcowych maksymalnych kolokacji spośród kandydatów wyznaczonym w~poprzednim procesie. Na początku dla każdego z nich uruchamiany jest algorytm wyszukiwania klik instancji. Do jego zrozumienia niezbędne jest wprowadzenie poniższych definicji.

\begin{defin}[Uporządkowana klika instancji]
Dana jest tabela instancji kolokacji o rozmiarze 2 ($ InsTable_{2}) $ oraz kandydat na maksymalną kolokację $ C_{m} $. Jego uporządkowana klika instancji $ InsC_{m} $ jest grupą instancji przestrzennych spełniających następujące warunki:
\begin{itemize}
\item rozmiar $ InsC_{m} $ jest równy rozmiarowi $ C_{m} $, a cechy w odpowiadających sobie instancjach w $ InsC_{m} $ i $ C_{m} $ są takie same; %a każdej instancji w $ InsC_{m} $ odpowiada cesze w odpowiadającej jej instancji w $ C_{m} $; %
\item instancje każdej pary instancji w $ InsC_{m} $ sąsiadują ze sobą w przestrzeni i~można je znaleźć w tabeli  $ InsTable_{2} $.
\end{itemize}
\end{defin}

\begin{defin}[Skondensowane drzewo instancji]Dany jest kandydat na maksymalną kolokację $ C_{m} $. Skondensowane drzewo instancji $ CInsTree$ jest konstrukcją kompresującą wszystkie uporządkowane kliki instancji $ C_{m} $.
\label{def:condensed}
\end{defin}

Algorytm~\ref{alg:chinczyki_step41} ma charakter iteracyjny, w którym zmienna $ i $ jest zmienną sterującą pętli. Na początku zostaje zainicjalizowane drzewo $ CInsTree $ i tworzony jest jego korzeń. Następnie uruchamiany jest proces konstrukcji drzewa, podzielony na dwa etapy. Pierwszy etap wykonywany jest tylko w pierwszej iteracji algorytmu - kolejne iteracje będą wykonywać krok drugi. 

\begin{algorithm}
\SetKwInOut{Input}{Wejście}
\SetKwInOut{Output}{Wyjście}
\SetKwProg{myproc}{Procedure}{}{}
\SetKwFunction{proc}{BK\_Pivot}
\Input{$C_{m}, InsTable_{2}$}
\Output{$CInsTree$ - skondensowane drzewo instancji $ C_{m} $}
$ i \leftarrow 1; CInsTree \leftarrow \emptyset$; utwórz korzeń drzewa $ CInsTree $\;
\While{$ i < size(C_{m}) $}{
	\eIf{$ i = 1 $}{
		\ForEach{para instancji $ InsPair_{k} \in InsTable_{2}(C_{m}(1),C_{m}(2))$}{
			\eIf{$ InsPair(1) \in CInsTree_{0}$.children}{
				dodaj węzeł podrzędny $InsPair_{k}(2)$ do $CInsTree_{1}(InsPair_{k}(1))$\;
			}{
				utwórz poddrzewo z $ InsPair_{k}(1) $ jako korzeniem i $ InsPair_{k}(2) $ jako pierwszym dzieckiem\;
				dołącz to podrzewo do korzenia $ CInsTree $\;
			}
		}
	}{
		\ForEach{węzeł instancji $ ins_{k} \in CInsTree_{i}$}{
			znajdź indeksy elementów równych $ ins_{k} $ od pierwszej kolumny $ InsTable_{2}(C_{m}(i), C_{m}(i + 1))$\;
			przechowaj drugi element odpowiadającej pary instancji w liście $El$\;
			\ForEach{$ei_{t} \in El$}{
				$ flag \leftarrow i - 1 $\;
				$ currIns \leftarrow ins_{k}$.parent\;
				\While{$ flag \geq 1 $}{
					\eIf{$(currInt, ei_{t}) \in InsTable_{2}(C_{m}(flag),C_{m}(i + 1))$}{
						$ currIns \leftarrow currIns$.parent\;
					}{break;}
					$ flag \leftarrow flag - 1 $\;
				}
				\lIf{$ flag = 0 $}{dodaj węzeł podrzędny $ ei_{t}$ do $ CInsTree_{i}(ins_{k})$}
			}
		}
	}			
}
\caption{Konstrukcja skondensowanego drzewa instancji $ C_{m} $}
\label{alg:chinczyki_step41}
\end{algorithm}

W pierwszym kroku (linie 3-11) dla każdej pary instancji pierwszych dwóch cech przestrzennych $ C_{m} $ nastepuje sprawdzenie, czy pierwszy element aktualnie przetwarzanej pary instancji istnieje na danym poziomie drzewa $ CInsTree $ (oznaczonego jako $CInsTree_{1}$). Jeżeli tak, następuje dodanie drugiego elementu jako węzeł podrzędny odpowiadającego mu węzła na poziomie pierwszym. W przeciwnym wypadku, na podstawie obecnej pary instancji tworzone jest poddrzewo, które następnie jest dołączane do korzenia $ CInsTree $. 

Drugi etap rozpoczyna się konstrukcją listy $ El$ zawierającej instancje cechy $ C_{m}(i+1) $. Dokonuje się tego dla każdego węzła instancji na poziomie $i$-tym drzewa $CInsTree$ popzez skanowanie $ InsTable_{2}(C_{m}(i),C_{m}(i+1))$, gdzie $ c_{m}(i)$ jest i-tą cechą $ C_{m} $. Nastepnie dla każdego elementu tej listy nastepuje sprawdzenie, czy zarówno para instancji składająca się z tego elementu, jak i każdy przodek aktualnego węzła instancji $ CInsTree$ znajduje się w tablicy kolokacji o długości 2 - $InsTable_{2}$. Jeżeli tak, dodaje się ten element jako węzeł podrzędny aktualnego węzła instancji.

Proces działa do czasu, gdy nie będzie żadnego węzła na $ i $-tym poziomie drzewa lub dopóki $i$ nie będzie mniejsze niż $ len(C_{m}) - 1 $.

Po zakończeniu algorytmu pozostaje jeszcze wyliczenie indeksów powszechności dla odnalezionych klik instancji. Podobnie jak wcześniej, w przypadku kiedy miara powszechności nie jest mniejsza niż przyjęty na początku próg powszechności, kandydata można przyjąć jako właściwy wzorzec kolokacji \cite{huang}.

\newpage

\section{Implementacja CPU}
\label{sec:cpu}

W tym rozdziale zostaną opisane implementacje rozwiązania problemu \cite{chinczyki} wykorzystujące wyłącznie potencjał procesorów CPU (ang. \textit{Central Procession Unit}), bez udziału karty graficznej w obliczeniach. W szczególności zostaną zaprezentowane techniki optymalizacyjne dedykowane dla poszczególnych podejść do rozwiązania problemu. 

Do celów porównawczych zostały zrealizowane dwa rozwiązania: sekwencyjne (bez żadnego wsparcia dla równoległości) oraz równoległe (z wykorzystaniem biblioteki \textit{Parallel Patterns Library} firmy \textit{Microsoft}). 

\subsection{Wersja sekwencyjna}

\subsubsection{Generowanie instancji w oparciu o próg odległości}
\label{subsec:cpu-step-1}

Podobnie jak w pierwotnej wersji algorytmu, generowanie kandydatów na kolokacje o rozmiarze 2 następuje na podstawie \textit{łączenia przestrzennego w oparciu o~zamiatanie} (ang. \textit{sweeping-based spatial join}). Algorytm zamiatania (ang. \textit{plane sweep algorithm}) użyty w procesie łączenia został jednak nieznacznie zmodyfikowany.

Na początku wektor będący zbiorem wejściowym jest traktowany algorytmem \textit{sortowania szybkiego} (ang. \textit{quick sort}) dostępnym w bibliotece standardowej C++. Jako kryterium sortowania zostaje wybrana relacja między arbitralnie wybranym wymiarem przestrzeni.

Następnie następuje wyznaczenie progu odległości, na podstawie którego będą dobierani kandydaci na kolokacje. Warto zwrócić w tym miejscu uwagę na fakt, że algorytmowi nie jest potrzebna wiedza o rzeczywistej odległości między dwoma punktami w przestrzeni, a jedynie relacja między tą odległością a zadanym progiem odległości. Daje to możliwość uniknięcia niepotrzebnych obliczeń (pierwiastkowania) przy wyznaczaniu odlegości między punktami. Dlatego też, zamiast bezpośrednio działać w oparciu o zadany próg odległości, wyliczany jest \textit{efektywny próg odległości}, będącym progiem odległości podniesionym do potęgi drugiej. 

Pozostało już jedynie filtrowanie obiektów w oparciu o wyliczony efektywny próg odległości. Dla każdego obiektu ze zbioru wejściowego następuje przeszukiwanie we wszystkich kierunkach obiektów sąsiadujących z nim, a następnie sprawdzana jest odległość między nimi. Gdy jest ona mniejsza niż ustalony wcześniej próg odległości obiekt umieszczany jest w specjalnej strukturze $ insTable $. Jednocześnie tworzony jest wektor $ typeIncidenceCounter$, w którym zliczane są wystąpienia kolejnych cech instancji przestrzennych (informacja o tym przyda się przy wyznaczaniu miary powszechności). Dodatkowo, pętla wewnętrzna jest przerywana, jeżeli wartość bezwzględna różnicy posortowanych współrzędnych poszczególnych obiektów będzie większa niż zadany próg odległości. Ma to na celu ograniczenie zbędnych porównań w przypadku, kiedy istnieje pewność, że żadne kolejne punkty nie spełnią progu odległości - a zatem nie wejdą do zbioru $ insTable $.
 
Struktura $ insTable $ pełni podobną rolę, jak w oryginalnym algorytmie \textit{SGCT}, nie jest ona jednak dwuwymiarową tablicą z haszowaniem. Zamiast tego zastosowano trójwymiarową mapę wskaźników na wektor liczb, w której wymiarami są kolejno: typ elementu $ A $, typ elementu $ B $ oraz numer instancji przestrzennej $ A $.  W wektorze przechowywane są kolejne numery instancji przestrzennej \textit{B}. W celu efektywnego odczytywania sąsiadów o danej cesze konkretnej instancji, dane w wektorze są umieszczane w taki sposób, że zawsze spełniają relację \textit{numer cechy A} $ > $ \textit{numer cechy B}. Dodatkowo, zamiast standardowej mapy (\textit{std::map}) została wykorzystana mapa nieuporządkowana - gwarantuje ona większą wydajność przeglądania (kosztem większego zużycia pamięci).

\subsubsection{Filtrowanie sąsiadów w oparciu o próg minimalnej powszechności}

Krok ten stanowi zmodyfikowaną wersję algorytmu obliczania miar powszechności znanego z algorytmu \textit{Co-location Miner} (patrz Rozdział~\ref{subsec:clm}).

Na początku wykonywana jest funkcja $ countUniqueInstances$. Ma ona na celu zliczenie wszystkich wystąpień par instancji bez duplikatów. Podobnie jak w kroku pierwszym zostały tu użyte mapy nieuporządkowane (\textit{std::unordered\_map}) w celu przyspieszenia przetwarzania.
Kluczem w tej mapie mogą być zarówno pary, jak i~krotki.

Do tworzenia map nieuporządkowanych zazwyczaj stosuje się \textit{sumę modulo 2}~jako funkcję mieszania - jednak w praktyce takie rozwiązanie generuje zbyt dużą ilość kolizji. W związku z tym, do budowy mapy została użyta własna funkcja mieszająca (patrz Algorytm~\ref{alg:pairHashFunction}) powstała na bazie \textit{hash\_combine} z biblioteki \textit{boost}. Ogranicza ona występowanie kolizji w przypadku wstawiania nowych elementów do tablicy z~haszowaniem. 

\begin{algorithm}
\SetKwInOut{Input}{Wejście}
\SetKwInOut{Output}{Wyjście}
\SetKwProg{myproc}{Procedure}{}{}
\SetKwFunction{proc}{hash\_combine}
\Input{ $p$ - para zawierająca elementy $ first $ oraz $ second $ dające się hashować }
\Output{ $seed$ - wartość funkcji mieszającej dla pary $p$ }
\myproc{\proc{seed, key}}{
$ seed \leftarrow seed \oplus (f_{hash}(key) + $ 0x9e3779b9 $ + (seed << 6) + (seed >> 2))$\;
}
$ seed_1 \leftarrow 0 $\;
$ seed_1 \leftarrow $ hash\_combine $(seed_1, p.first) $\;
$ seed_1 \leftarrow $ hash\_combine $(seed_1, p.second) $\;

$ seed_2 \leftarrow 0 $\;
$ seed_2 \leftarrow $ hash\_combine $(seed_2, p.second) $\;
$ seed_2 \leftarrow $ hash\_combine $(seed_2, p.first) $\;

$ seed \leftarrow minimum(seed_1, seed_2) $\;

\caption{Funkcja mieszająca dla pary $p$}
\label{alg:pairHashFunction}
\end{algorithm}

Następnie w oparciu o wektor wynikowy tej funkcji oraz utworzony wcześniej wektor $ typeIncidenceCounter$ (patrz Rozdział~\ref{subsec:cpu-step-1}) dla każdego kandydata w tabeli $ insTable $ wyliczany jest współczynnik uczestnictwa. W przypadku, gdy obliczona miara powszechności jest mniejsza od zadanego progu minimalnej powszechności, kandydat jest usuwany z struktury $ insTable $, a użyta przez niego pamięć zostaje zwolniona na poczet dalszych obliczeń.

\subsubsection{Generowanie kandydatów na kolokacje maksymalne}
\label{subsec:kerbosz}

Tak jak w przypadku oryginalnego algorytmu, kandydaci na kolokacje maksymalne są generowani w oparciu o \textit{graf kolokacji o rozmiarze 2} (patrz Definicja~\ref{def:size2-col-graph}). Tworzony jest na bazie struktury $ insTable $ za pomocą funkcji \textit{createSize2ColocationsGraph}. Wykorzystywana jest w tym celu macierz sąsiedztwa (ang. \textit{adjacency matrix}). Krawędź między elementami pary instancji jest tworzona tylko i wyłącznie wtedy, kiedy numer instancji przestrzennej $ A $ jest większy od zera. 

\begin{algorithm}
\SetKwInOut{Input}{Wejście}
\SetKwInOut{Output}{Wyjście}
\Input{$G=(E,V)$}
\Output{$k$ (miara degeneracji), $L$ (lista wierzchołków uporządkowanych według miary degeneracji)}
$L \leftarrow \emptyset$\; 
$D \leftarrow \emptyset$\;
\ForEach{$v_{i} \in G $}{
$D(d_{v}) \leftarrow $ liczba sąsiadów $ v \notin L $ (początkowo równa stopniowi wierzchołka)\;
}
$k \leftarrow 0$\;
\ForEach{$v_{i} \in G $}{
znajdź takie $ i $, dla którego $ D(i) \neq \emptyset $\;
$ k \leftarrow max(k, i)$\;
$ v \leftarrow $ wierzchołek z $ D(i)$\;
$ L \leftarrow \{v\} \cup L $\;
$ D(i) \leftarrow D(i) \backslash \{v\}$\;
\ForEach{$ w \leftarrow $ sąsiedzi $ v \notin L $}{
$ d^{'}_{w} \leftarrow d_{w} - 1$\;
$ D(d_{w}) \leftarrow D(d_{w}) \backslash \{w\}$\;
$ D(d^{'}_{w}) \leftarrow D(d^{'}_{w}) \cup w $\;
}
}
\caption{Obliczanie miary degeneracji metodą Matuli i Becka (1983)}
\label{alg:matusiak}
\end{algorithm}

Po wygenerowaniu grafu liczona jest jego miara degeneracji (patrz Definicja~\ref{def:degeneracy}). W tym celu został wykorzystany Algorytm~\ref{alg:matusiak} zaprezentowany przez Matulę i Becka w pracy \cite{matusiak}. Działa on w czasie wielomianowym, co pozwala na odciążenie CPU. Funkcja zwraca zarówno miarę degeneracji, jak uporządkowany według niej wektor wierzchołków występujących w grafie.

Dalsze kroki algorytmu wykonywane są dla poszczególnych wierzchołków uporządkowanych według miary degeneracji. Podobnie jak w oryginalnym algorytmie z~pracy \cite{pivot} tworzone są grupy wierzchołków o niższych i wyższych indeksach, a~następnie uruchamiana jest funkcja $ BK\_Pivot $, w której następuje wyszukiwanie maksymalnych klik w oparciu o algorytm Brona-Kerboscha \cite{kerbosz}. 

Aby ograniczyć czas wyszukiwania maksymalnych klik, zostały zastosowane tzw. \textit{wektory sortowane} zamiast zazwyczaj używanych zbiorów (\textit{std::set}) z biblioteki C++. Posiadają one większą wydajność, a do tego pozwalają na skorzystanie z~funkcjonalności zarezerwowanej wyłącznie dla zbiorów.

\subsubsection{Generowanie skondensowanych drzew instancji oraz filtrowanie kandydatów na podstawie progu powszechności}

Na początku wszystkie rozważane maksymalne kliki z poprzedniego kroku są umieszczane w specjalnej strukturze $ cliquesToProcess $ będącej trójwymiarowym wektorem, gdzie pierwsza współrzędna stanowi rozmiar kolokacji, a druga o pozycji kliki w kolejce w poszczególnych wierszach. Trzecią współrzędną są oczywiście kolejne elementy danej kliki.

Następnie iteracyjnie przetwarzana jest każda klika, począwszy od największej. Zrezygnowano w tym miejscu z podejścia rekurencyjnego, gdyż prowadziłoby to do niepotrzebnego utrzymywania ogromnego stosu wywołań (ang. \textit{call stack}). W każdej iteracji sprawdzana jest miara powszechności dla kliki instancji - odpowiedzialna jest za to funkcja $ isCliquePrevalent $. 

Wyliczone kliki są umieszczane w obiektach \textit{CliqueContainer} i \textit{LapsedCliqueContainer}. Pełnią one rolę pamięci podręcznej dla powyższych algorytmów. Dzięki temu nie ma potrzeby ponownego przeliczania tych samych klik instancji, co prowadzi do kilkunastokrotnego wzrostu wydajności obliczeń.

Dla klik o długości większej niż 2 tworzone jest skondensowane drzewo instancji (patrz Definicja~\ref{def:condensed}). Implementacja konstrukcji takiego drzewa zasadniczo nie różni się od pierwotnego algorytmu zaprezentowanego w pracy \cite{chinczyki}.

W celu ograniczenia przeszukiwań drzewa w dół zastosowano dodatkowe wskaźniki do rodziców. Zdefiniowany został także dodatkowy wektor zawierający wskaźniki do liści znajdujących się na ostatnim poziomie drzewa (wyznaczanym na bazie rozmiaru aktualnie budowanej kliki - w przypadku, kiedy drzewa nie da się rozbudować o kolejny poziom, wektor ten pozostaje pusty). Podejście to pozwala na szybki dostęp do końcowych instancji poszczególnych kandydatów na instancje.

Na końcu zliczane są wystąpienia instancji w klikach za pomocą odwróconej pętli i na ich podstawie wyliczana jest miara powszechności. Początkowo iteracje następują po poszczególnych cechach kolejnych wektorów.

W przypadku, kiedy miara nie jest mniejsza niż przyjęty na początku próg powszechności, kandydat jest dodawany do zbioru rozwiązań. W przeciwnym wypadku do struktury $ cliquesToProcess $ dodawane są podkliki, które również będą rozważane jako potencjalni kandydaci na kolokacje.

\subsection{Wersja wielowątkowa}

Jak już wcześniej wspomniano, rozwiązanie równoległe zostało oparte o multiplatformową, wysokopoziomową bibliotekę \textit{Parallel Patterns Library} firmy \textit{Microsoft}. Została ona pierwszy raz zaprezentowane szerszej publiczności wraz z wydaniem środowiska \textit{Visual Studio 2010} i docelowo ma być konkurencją dla popularnej biblioteki \textit{OpenMP}. 

Cechą charakterystyczną tej biblioteki jest podobieństwo składni do tej z biblioteki standardowej C++ (ang. \textit{C++ Standard Library}). Wykorzystuje też wszystkie właściwości języka C++, jakie przynosi standard C++11 i późniejsze (w tym funkcje \textit{lambda}).

Biblioteka \textit{PPL} wprowadza zbiór obiektów zwanych \textit{kontenerami o dostępie równoległym} (ang. \textit{concurrent containers}). Są to odpowiedniki kontenerów z biblioteki standardowej C++, które cechują się równoległymi wersjami funkcji operujących na kontenerach (np. operacji wstawiania czy usuwania). W zdecydowanej większości przypadków ich użycie nie różni się od wersji sekwencyjnych dostępnych w bibliotece STD - wymagane jest jedynie umieszczenie ich w odpowiednim bloku, np. \textit{parallel\_for}. 

W przypadku, kiedy istnieje potrzeba dostarczenia kopii kontenera dla każdego z~wątków (np. żeby nie blokować dostępu innym wątkom do współdzielonego obiektu), istnieje także klasa kontenerów typu \textit{combinable}. Kiedy równoległe przetwarzanie zostanie zakończone, prywatne kopie wygenerowane dla każdego z wątków są wtedy przezroczyście łączone w całość. Oczywiście \textit{PPL} zawiera także tradycyjne metody zapewnienia równoległości w programie, takie jak zadania (ang. \textit{tasks}) oraz klasyczne mutexy.

Zasadniczą różnicą między \textit{PPL} a \textit{OpenMP} jest zastosowanie dynamicznego planisty (ang. \textit{dynamic scheduler}), co pozwala na lepszą optymalizację równoległego przetwarzania w zależności od aktualnie dostępnych zasobów w systemie. Na dynamicznym planiście zyskują szczególnie problemy o charakterze rekurencyjnym (np. algorytmy sortujące czy szeroko wykorzystywane w niniejszej pracy przeszukiwanie danych) \cite{mikromiekki}. Do tego technologia \textit{OpenMP} nie zawiera żadnego mechanizmu anulowania, często wymaganego w algorytmach o charakterze równoległym \cite{stak}.

Powyższe wady środowiska \textit{OpenMP} były powodem, dla których ostatecznie - pomimo ubogiej dokumentacji oraz niskiej popularności -  została wybrana biblioteka \textit{Parallel Patterns Library} jako najbardziej optymalna dla algorytmu \textit{SGCT} będącego tematem tej pracy.

\subsubsection{Generowanie sąsiadów na podstawie progu odległości}

Różnice w równoległej implementacji generowania sąsiadów w stosunku do wersji sekwencyjnej (patrz Rozdział~\ref{subsec:cpu-step-1}) wynikają w dużej mierze z zastosowania alternatywnych struktur dostępnych w ramach biblioteki \textit{Parallel Pattern Library}. 

Dla przykładu, zbiór wejściowy jest sortowany za pomocą wbudowanego w bibliotekę PPL algorytmu \textit{równoległego sortowania buforowanego} (ang. \textit{parallel buffered sort}). Jest to odmiana standardowego sortowania szybkiego wykorzystujące równoległe scalanie (ang. \textit{parallel merge}) w celu zastąpienia pierwotnego kroku dzielenia w algorytmie sortowania szybkiego, którego nie da się zrównoleglić \cite{buffered-sort} - i podobnie jak \textit{quick sort}, po posortowaniu elementy o tej samej wartości mogą nie występować w tej samej kolejności (przykład tzw. \textit{sortowania niestabilnego}). Dla wielu procesorów pozwala osiągnąć większą wydajność niż standardowy \textit{quick sort} (jego złożoność wynosi $ O((n/p)*\log{n}) $ dla $ p $ procesorów). Podejście to wymaga niestety dodatkowej alokacji pamięci o rozmiarze $ O(n) $. 

W procesie filtrowania obiektów na podstawie wyliczonego progu odległości, podobnie jak w wersji sekwencyjnej, tworzona jest tablica asocjacyjna $ insTable $ -  jednak w podejściu równoległym jest ona typu \textit{combinable}. Dzięki temu każdy wątek posiada swoją prywatną kopię tablicy $ insTable $, w której umieszcza przetwarzane przez siebie pary instancji, które nie przekraczają wyliczonego progu efektywnej odległości. Po zakończeniu przetwarzania przez wszystkie wątki lokalne kopie struktury $ insTable $ są łączone w całość poprzez zastosowanie operacji redukcji oraz sumowania wektorów parami, co eliminuje problem współbieżnego dostępu do struktury w środowisku równoległym. Podobnie sytuacja wygląda w przypadku struktury $ typeIncidenceCounter $.

W algorytmie filtrowania został wykorzystany także dynamiczny planista (\textit{auto partitioner}). W przypadku przerwania pętli wewnętrznej algorytmu, planista dynamicznie przydziela bezczynnym wątkom kolejne dane wejściowe niezbędne do dalszego przetwarzania. Po wyjściu z pętli następuje synchronizacja wątków, a tablica $ insTable $ ulega scaleniu.

Poza powyższymi usprawnieniami, ogólny mechanizm oraz struktury zastosowane w kroku pierwszym pozostały właściwie niezmienione w stosunku do wersji sekwencyjnej. 

\subsubsection{Filtrowanie sąsiadów na podstawie progu minimalnej powszechności}

Jedyną różnicą na tym etapie algorytmu w stosunku do odmiany sekwencyjnej jest dodatkowy krok określenia zakresu pracy dla każdego z wątków biorących udział w liczeniu wystąpień par instancji. Odpowiedzialna za to jest funkcja $ getWorkloadForInsTable $.

W porównaniu do poprzedniego etapu, wykorzystywany jest statyczny planista (ang. \textit{static-partitioner}) będący odpowiednikiem funkcji \textit{omp parallel for} z biblioteki \textit{OpenMP}. Dzięki temu każdemu wątkowi przydzielana jest stała ilość par instancji do przetworzenia. Liczba ta jest wyznaczana na podstawie rozmiaru tablicy $ insTable $. Daje to pewną oszczędność czasu, gdyż wątki nie tracą czasu na dynamiczne przydzielanie pracy do wykonania.

\subsubsection{Generowanie kandydatów na kolokacje maksymalne}

Zasadniczą zmianą na tym etapie algorytmu jest wprowadzenie równoległej wersji algorytmu \ref{alg:bk3} liczenia maksymalnych klik metodą Brona-Kerboscha \cite{kerbosz}. Korzysta on ponownie z dynamicznego planisty w celu dynamicznego rozdzielenia pracy dla poszczególnych wątków, ponieważ nieznany jest czas wykonywania algorytmu.

Każdy z wątków uruchamia dla swojej puli wierzchołków standardową wersję algorytmu Brona-Kerboscha wykorzystującą tzw. \textit{pivot}. Podejście to może powodować występowanie duplikatów - dlatego też po zakończeniu działani algorytmów zbiór wynikowy rzutowany jest na kontener \textit{std:set}, likwidując tym samym potencjalnie występujące w zbiorze duplikaty. 

Tradycyjnie na końcu lokalne listy maksymalnych klik są scalane w jedną strukturę wynikową za pomocą instrukcji \textit{combinable::combine\_each}.
\begin{algorithm}

\SetKwInOut{Input}{Wejście}
\SetKwInOut{Output}{Wyjście}
\Input{$G=(E,V)$}
$R \leftarrow \emptyset; X \leftarrow \emptyset; P \leftarrow V(G)$\;
\ForEach{$v^{*}_{i}$ in lista $v^{*}_{1}, v^{*}_{2},..., v^{*}_{\lambda}$  uporządkowanych wg. degeneracji}{
$BK\_Pivot( R \cup \{ v^{*}_{i}\}, P \cap N(v^{*}_{i}), X \cup N(v^{*}_{i})$\;
$ P \leftarrow P \backslash \{v^{*}_{i}\} $\;
$ X \leftarrow X \cup \{ v^{*}_{i}\} $\;
}
\caption{Równoległy algorytm Brona-Kerboscha}
\label{alg:bk3}
\end{algorithm}

\subsubsection{Generowanie skondensowanych drzew instancji oraz filtrowanie kandydatów na podstawie progu powszechności}

W porównaniu do wersji sekwencyjnej, konstrukcja skondensowanego drzewa instancji oraz liczenie miary powszechności jest dokonywany w sposób równoległy. Wymagało to zaprojektowania równoległych odpowiedników kontenerów znanych z~odmiany sekwencyjnej, czyli \textit{ParallelCliqueContainer} i \textit{ParallelLapsedCliqueContainer} zapewniających bezpieczeństwo przetwarzania równoległego. W tym celu zastosowano mechanizm sekcji krytycznych w procesie inkrementacji liczników, a także kontenery o dostępie równoległym. Oprócz zapewnienia własności bezpieczeństwa przełożyło się to na zwiększenie efektywności przetwarzania.

Niezbędne było także zastosowanie (podobnie jak w kroku pierwszym) dynamicznego planisty (ang. \textit{auto scheduler}) przydzielającego dynamicznie kolejne kliki do przetworzenia do poszczególnych wątków. Niemożliwe było zastosowanie statycznego planisty z uwagi na fakt, że na tym etapie nie można przewidzieć dokładnego czasu konstrukcji skondensowanego drzewa instancji.

Dla struktury \textit{ParallelCliqueContainer} konieczne okazało się zaimplementowanie funkcji mieszającej dla całych wektorów, ponieważ zbiór równoległy (ang. \textit{concurrent set}) dostępny w bibliotece \textit{PPL} wykorzystuje \textit{hashe} w procesie dodawania elementów (odmiennie niż pierwowzór z biblioteki STD korzystający z operacji porównania). W tym celu skorzystano z funkcji \textit{hash\_combine} (patrz Algorytm~\ref{alg:vector_hash_combine}).

\begin{algorithm}
\SetKwInOut{Input}{Wejście}
\SetKwInOut{Output}{Wyjście}
\SetKwProg{myproc}{Procedure}{}{}
\SetKwFunction{proc}{hash\_combine}
\Input{ $ vec $ - wektor źródłowy }
\Output{ $seed$ - wartość funkcji mieszającej }
\myproc{\proc{seed, key}}{
$ seed \leftarrow seed \oplus (f_{hash}(key)) + $ 0x9e3779b9 $ + (seed << 6) + (seed >> 2))$\;
}
$ seed \leftarrow |vec| $\;
\ForEach{$ i \in vec $}{
$ seed \leftarrow $ \textit{hash\_combine} $(seed, i)$\;
}
\caption{Algorytm operatora () struktury \textit{vector\_hash}}
\label{alg:vector_hash_combine}
\end{algorithm}

Oddzielną kwestią jest struktura tablicy $ insTable $. Podobnie jak w wersji sekwencyjnej zawiera trzy wymiary. Wykorzystano wektor pochodzący z standardowej biblioteki C++, którego elementy są wskaźnikami na wektory równoległe (ang. \textit{concurrent vector}) pochodzące z biblioteki \textit{PPL} - same z kolei zawierają wskaźniki na wektory klasyczne. To nieco pokrętne rozwiązanie zapewnia bezproblemowe przetwarzanie równoległe, a także - ze względu na minimalizację fragmentacji zaalokowanej pamięci - przyspiesza działanie algorytmu.

Podobnie jak w innych krokach, również tutaj wykorzystywany jest wektor typu \textit{combinable}. Umieszczany jest w niej ostateczny wynik przetwarzania będący scaleniem lokalnych kopii tablic posiadanych przez każdy wątek, biorący udział w~wyznaczaniu ostatecznych kandydatów na kololacje.

\newpage

\section{Implementacja GPU}
\label{sec:gpu}

Niniejszy rozdział poświęcony jest implementacji algorytmu \textit{SGCT} w wersji na procesory graficzne. Jak już wcześniej opisano w Rozdziale~\ref{sec:cuda}, w tym celu zostało wykorzystane środowisko programistyczne \textit{CUDA Toolkit} w wersji \textit{8.0}.

Poniższa implementacja została opracowana dla urządzeń spełniających wersję 3.0 potencjału obliczeniowego CUDA (ang.\textit{ CUDA Compute Capability}). W związku z tym, nie wykorzystuje ona niektórych możliwości dostarczanych przez nowsze karty graficzne firmy NVIDIA, np. zunifikowanej pamięci współdzielonej (ang. \textit{Unified Memory} \cite{cuda_um}) czy technologii \textit{Dynamic Parallelism} \cite{cuda_dp} - pozwoliło to jednak na przeprowadzenie testów na większej ilości kart graficznych.

%\subsection{Przygotowanie danych w pamięci GPU}

%Z powodu rozdzielnego modelu pamięci (co zmienia się wraz z wprowadzeniem \textit{unified memory}) dane które zostały %wczytane z zewnętrznego źródła (zbiór instancji zawierających typ, identyfikator, położenie w przestrzeni dwuwymiarowej) %%%(//TODO napisać gdzieś o możliwościach rozszerzenia algorytmu) muszą zostać przekazane do pamięci dostępnej dla procesorów %CUDA. Operacja przenoszenia danych pomiędzy przestrzeniami pamięci jest dość kosztowna i silnie zależy od wybranych %rozwiązań sprzętowych.

\subsection{Generowanie tabeli instancji o rozmiarze 2}

Tak jak w przypadku implementacji na procesory CPU, generowanie kandydatów na kolokacje o rozmiarze 2 wykorzystuje algorytm \textit{łączenia przestrzennego w oparciu o zamiatanie} (ang. \textit{sweeping-based spatial join}), przeznaczony do uruchamiania w~architekturze masowo-równoległej. 

Na początku niezbędne jest wczytanie danych z zewnętrznego źródła do struktur znajdujących się w pamięci karty graficznej - wynika to z zastosowanego w projekcie \textit{Compute Capability}, który wymusza rozdzielny model pamięci. Operacja ta należy do dość kosztownych, a do tego w dużym stopniu zależy od zastosowanego sprzętu.

Wczytany zbiór wejściowy zostaje potem posortowany względem osi odciętych w kolejności rosnącej. Takie rozwiązanie pozwala na ograniczenie obszaru przeszukiwania sąsiadów instancji. Do tego celu zostały zastosowane funkcje sortujące wbudowane w bibliotekę \textit{Thrust}.

Następnie zostaje uruchomiony właściwy \textit{algorytm zamiatania} (ang. \textit{plane sweep}). Na początku zostaje uruchomiony kernel $ countNeighbours $ wyznaczający liczbę sąsiadów dla każdej instancji. W dużym skrócie polega on na obliczeniu odległości euklidesowej pomiędzy instancjami $ a_{i} $ oraz $ a_{k} $. Jeśli jest ona nie większa od założonej maksymalnej odległości pomiędzy instancjami \textit{distanceTreshold}, stwierdza się pomiędzy nimi relację sąsiedztwa. 

Na potrzeby tego kroku zostaje zarezerwowane $ n_{dt} \cdot 32 $ wątków, co daje jeden \textit{warp} na instancję. Każdy z nich wykonuje maksymalnie $\lceil i / 32\rceil - 1$ iteracji, w każdej \textit{j-tej} iteracji sprawdzana jest natomiast grupa 32 instancji z zakresu $ K=\{i - j~\cdot~32; i - (j + 1) \cdot 32\} $. Dodatkowo należy zaalokować tablicę $ T_{counts} $ o rozmiarze $ n_{dt} $, w której na pozycji $ i $ zostanie wpisana ilość sąsiadów instancji $ a_{i} $.

Każdy wątek wchodzący w skład \textit{warp}'u sumuje w tablicy pamięci współdzielonej liczbę znalezionych we wszystkich iteracjach sąsiadów. Suma ta jest przechowywana przez każdy wątek w osobnej komórce - pozwala to uniknąć stosowania kosztownego mechanizmu synchronizacji zapisu pomiędzy wątkami w \textit{warpie}. Po każdej iteracji następuje sprawdzenie, czy którykolwiek z wątków \textit{warp}'a napotkał instancję, która odległa jest na samej osi odciętych o więcej niż próg odległości \textit{distanceTreshold} - jeżeli tak, to można przerwać pętlę, gdyż kolejne instancje w posortowanym zbiorze na pewno nie spełnią progu odległości.

Po zakończeniu iterowania każdy \textit{warp} wykonuje szybką redukcję części tablicy współdzielonej należącej do jego wątków, a wynik wpisuje do tablicy \textit{$ T_{counts} $} na pozycji $i$-tej. Dodatkowo, po zakończeniu działania kernela  \textit{countNeighbours} również \textit{T\textsubscript{counts}} ulega redukcji - uzyskując tym sposobem całkowitą ilość znalezionych relacji sąsiedztwa sposród wszystkich instancji.

% (otrzymując całkowitą ilość znalezionych relacji sąsiedztwa wśród wszystkich instancji, oznaczmy tę wartość jako %\textit{nC}.

\begin{figure}[H]
\centering
\includegraphics{planesweep}
\caption{Poglądowy rysunek przedstawiający iterowanie warpa po instancjach}
\end{figure}

Pozostało jedynie wykonać kernel \textit{findNeighbours}, odpowiedzialny za alokację oraz wpisanie znalezionych sąsiadów do tabel \textit{InstanceFirst} oraz \textit{InstanceSecond}. Służą one do przechowywania elementów par instancji będących w relacji sąsiedztwa. Wartości należące do jednej pary znajdują się pod tymi samymi indeksami w obu tablicach. Kolejność w obrębie pary wyznaczana jest przez poniższy Algorytm~\ref{alg:FeatureInstance_operator} będący przeciążeniem operatora mniejszości ($ < $):

\begin{algorithm}
\SetKwInOut{Input}{Wejście}
\SetKwInOut{Output}{Wyjście}
\SetKwProg{myproc}{Procedure}{}{}
\SetKwFunction{proc}{operator<}
\Input{$a,b$ - instancje typu \textit{FeatureInstance}}
\Output{$ out $ - wynik porównania}
\myproc{\proc{a, b}}{
$ out \leftarrow a.field < b.field $\;
}
\caption{Operator mniejszości dla instancji typu \textit{FeatureInstance}}
\label{alg:FeatureInstance_operator}
\end{algorithm}

Przed uruchomieniem kernela niezbędne jest zaalokowanie tablic $ Pairs_{A} $ oraz $ Pairs_{B}$ o długości odpowiadającej wyznaczonej w wyniku redukcji tablicy $ T_{counts} $ liczbie odnalezionych relacji sąsiedztwa. Dodatkowo należy zaalokować tablicę $ Begins $ wielkości $ n_{dt} $ - przechowa ona wynik operacji sumy ekskluzywnej (ang. \textit{exclusive sum}) tablicy $ T_{counts} $. Struktura ta będzie wykorzystana w trakcie wyznaczania pozycji zapisu relacji w tablicach wynikowych.

Konfiguracja uruchomieniowa kernela \textit{findNeighbours} jest identyczna z konfiguracją użytą dla \textit{countNeighbours}. Sama funkcja różni się zresztą od poprzedniczki tylko i wyłącznie w miejscu stwierdzenia relacji sąsiedztwa - kernel \textit{findNeighbours} zamiast zliczać relacje sąsiedztwa wpisuje do tablic $ Pairs_{A} $ i $ Pairs_{B}$ instancje, pomiędzy którymi stwierdzono to sąsiedztwo. 

Pozycja zapisu w tablicach wynikowych wyznaczana jest przy pomocy operatora mniejszości (Algorytm~\ref{alg:FeatureInstance_operator}) dla obiektów typu \textit{FeatureInstance} w oparciu o:

\begin{itemize}
\item wartości w tablicy $ Begins $ na pozycji $i$,
\item ilości relacji sąsiedztwa znalezionych przez \textit{warp} w poprzednich iteracjach,
\item ilości relacji sąsiedztwa znalezionych w obecnej iteracji przez wątki \textit{warpa} o~mniejszym identyfikatorze. 
\end{itemize}

Warto zauważyć, że nigdzie nie jest zapisywana informacja o współrzędnych instancji - nie będą one potrzebne w dalszym toku pracy algorytmu.

\subsection{Uporządkowanie zbioru par}
\label{subsec:pair_ordering}

Struktury pomocnicze wykorzystywane w następnych krokach algorytmu wymagają, aby zbiór par został odpowiednio uporządkowany. W związku z tym, na zbiór par nałożony zostaje porządek wyznaczony przez Algorytm~\ref{alg:FeatureInstancePair-operator} będącym operatorem mniejszości zaimplementowanym dla par instancji typu $ FeatureInstance $.

\begin{figure}[H]
\begin{algorithm}[H]
\SetKwInOut{Input}{Wejście}
\SetKwInOut{Output}{Wyjście}
\SetKwProg{myproc}{Procedure}{}{}
\SetKwFunction{proc}{operator<}
\Input{ $ a, b $ - strony operatora typu \textit{FeatureInstance}}
\Output{ $ out $ - wynik porównania }
\myproc{\proc{a, b}}{
\uIf{$ featureId_{a_{0}} < featureId_{b_{0}} $}{
$ out \leftarrow True $\;
}
\uElseIf { $ featureId_{a_{0}} = featureId_{b_{0}} $ } {
      \uIf{$ featureId_{a_{1}} <\ b[1].fields.featureId $}{
   		$ out \leftarrow True $\;
	  }\uElseIf {$ featureId_{a_{1}} = featureId_{b_{1}} $} {
	  	\uIf{$ instanceId_{a_{0}} < instanceId_{b_{0}} $}{
   			$ out \leftarrow True $\;
	  	}\uElseIf {$ instanceId_{a_{0}}  = instanceId_{b_{0}} $} {
	  		 $ out \leftarrow instanceId_{a_{1}} = instanceId_{b_{1}}$\;
	  	}
	}
}
}
\caption{Operator mniejszości dla par instancji typu $ FeatureInstance $}
\label{alg:FeatureInstancePair-operator}
\end{algorithm}
\end{figure}

Przykładowe uporządkowanie przedstawia Rysunek~\ref{fig:pairs_content}. Tak posortowany zbiór posiada następujące właściwości:
\begin{itemize}
\item wszystkie relacje o tych samych typach zajmą w wejściowych tablicach ciągły obszar,
\item wszystkie instancje każdej relacji o określonej instancji pierwszego elementu oraz określonym typie drugiego elementu zajmą ciągłą przestrzeń w zbiorach $ Pairs_{A} $ oraz $ Pairs_{B} $.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics{sortAttr_insTable}
\caption{Przykładowa zawartość tablic \textit{Pairs}\textsubscript{A} oraz \textit{Pairs}\textsubscript{A}}
\label{fig:pairs_content}
\end{figure}

\subsection{Przygotowanie struktury szybkiego dostępu do informacji o instancjach należących do relacji o zadanych typach}

Na potrzeby następnych kroków algorytmu zdefiniowana została specjalna struktura $ ITMPack $. Ma ona na celu umożliwienie szybkiego dostępu do instancji, które należą do relacji o określonych cechach. 

Dzięki wspomnianej w poprzednim kroku właściwości obecnego uporządkowania zbioru par polegającej na zajmowaniu przez relacje o danych typach ciągłego obszaru można użyć funkcji \textit{reduce\_by\_key} dostępnej w bibliotece \textit{Thrust}, służącej do szybkiego wyznaczenia liczności unikalnych następujących po sobie elementów. Jako, że instancje w obecnym kroku rozróżnialne są tylko na podstawie typów zaimplementowany został algorytm~\ref{alg:FeatureInstanceTuple-operator} przeciążający operator równości ($ == $) wykorzystujący możliwość rozróżnienia instancji tylko względem identyfikatorów typów. Operator ten działa na strukturach \textit{FeatureInstanceTuple} - stanowią one parę obiektów \textit{FeatureInstance}, łączącą elementy z tablicy $ Pairs_{A} $ z odpowiadającymi im pozycją elementami tablicy $ Pairs_{B} $.

\begin{figure}[H]
\begin{algorithm}[H]
\SetKwInOut{Input}{Wejście}
\SetKwInOut{Output}{Wyjście}
\SetKwProg{myproc}{Procedure}{}{}
\SetKwFunction{proc}{operator==}
\Input{ $ a, b$ - strony operatora typu \textit{FeatureInstanceTuple}}
\Output{ $ out $ - wynik porównania }
\myproc{\proc{a, b}}{
$ out \leftarrow (featureId_{a_{0}} = featureId_{b_{0}}) \cup (featureId_{a_{1}} = featureId_{b_{1}})$\;
}
\caption{Operator równości dla par instancji typu \textit{FeatureInstanceTuple}}
\label{alg:FeatureInstanceTuple_operator}
\end{algorithm}
\end{figure}

Wynikiem działania funkcji \textit{reduce\_by\_key} są trzy tablice o jednakowym rozmiarze $ c_{r} $:
\begin{itemize}
\item \textit{Uniques} - zawiera pary typów, dla których istnieje przynajmniej jedna para instancji;
\item \textit{Counts} - zawiera liczności instancji relacji dla par typów instancji,
\item \textit{Begins} - zawiera indeksy tabeli instancji relacji, oznaczające początki obszarów ciągłych odpowiadającym danym parom typów instancji.
\end{itemize}

Na podstawie tych danych uzupełniana jest mapa struktury \textit{ITMPack}. W tym celu uruchamiany jest kernel uruchamiany z ilością wątków równym $ c_{r} $. Dla każdego $ i $-tego elementu tablic tworzy on strukturę \textit{InstanceTable}, uzupełniając ją \textit{i}-tymi wartościami z \textit{Counts} oraz \textit{Begins}, a następnie wstawia go do tablicy pod kluczem utworzonym z $i$-tego elementu tablicy \textit{Uniques} w postaci połączonej pary typów. 

\subsection{Generowanie prewalentnych relacji sąsiedztwa pomiędzy typami połączeń}

Mając gotową strukturę $ ITMPack $, można przystąpić do wyznaczenia par instancji, których współczynnik powszechności jest nie mniejszy niż zadany na początku próg powszechności. 

W tym celu niezbędne jest utworzenie struktur pomocniczych $ Counts_{A} $ i $ Counts_{B} $ przechowujących liczbę unikalnych wystąpień instancji odpowiednio pierwszego i~drugiego typu elementu relacji na odpowiadającej pozycji tablicy $ ITMPack.uniques $. Dla każdej z tych tabel obliczana jest liczność unikalnych instancji dla typów równych odpowiednio pierwszym i drugim elementom relacji z tablicy \linebreak $ ITMPack.uniques $. Liczność ta wyznaczana jest na podstawie tabel $ Pairs_{A} $ i $ Pairs_{B} $ w zakresach wyznaczonych z wartości zawierających się w tabelach $ ITMPack.begins $ oraz $ ITMPack.counts $. Tworzone są również podobne struktury przechowujące całkowitą liczbę instancji - noszą one odpowiednio nazwy $ TypesCounts_{A} $ dla pierwszego i $ TypesCounts_{B} $ dla drugiego elementu relacji.

Oprócz tego budowana jest tablica $ Mask $ wielkości $ N_{ItmUq} $ stanowiąca tzw. \textit{maskę prewalentności}. Początkowo zawiera ona wyłącznie wartości typu $ False $. Wartości tej tablicy wyznaczane są na podstawie dzielenia elementu tablicy $ Counts $ przez odpowiadający mu odpowiednik z tablicy $ TypesCounts $. Jeżeli minimum z ilorazów wyliczonych dla obu typów ($ A $ i $ B $) jest nie mniejsze niż zadany wcześniej próg minimalnej powszechności, na odpowiedniej pozycji w tablicy $ Mask $ umieszczana jest wartość $ True $.

Posiadając zbudowaną maskę prewalentności można już wygenerować pary instancji. Uruchamiany jest zatem kernel przez $ N_{ItmUq} $ wątków. Każdy z nich sprawdza, czy na swojej pozycji w tabeli $ Mask $ znajduje się wartość $ True $ - jeżeli tak, z~struktury $ WritePos $ przechowującą sumę ekskluzywną (ang. \textit{exclusive sum}) tabeli $ Mask $ pobiera $i$-tą wartość. Następnie następuje utworzenie struktury \linebreak $ FeatureTypePair $ w oparciu o relację z $ i $-tej pozycji w tabeli $ ITMPack.uniques$. Zostanie ona zapisana w tabeli $ PrevalentPairs $ przechowującej finalne pary instancji pod indeksem równym wartości pobranej z tabeli $ WritePos $.

\subsection{Generowanie kandydatów na kolokacje maksymalne}

W wersji GPU algorytmu do generowania kandydatów maksymalnych wykorzystany został mechanizm użyty w wersji CPU (patrz Rozdział~\ref{subsec:kerbosz}) -  w oparciu o~\textit{graf kolokacji o rozmiarze 2} (patrz Definicja~\ref{def:size2-col-graph}) oraz algorytm Brona-Kerboscha \cite{kerbosz}.

Różnicą względem algorytmu CPU jest to, że zbiór wynikowy kandydatów jest grupowany względem ich długości. Umożliwia to szybki dostęp do kandydatów o jednakowej długości w następnym kroku. Zbiór ten oznaczany jako $ uniqueCandidates $ stanowi mapę wektorów kandydatów, gdzie kluczem jest długość kandydata.

\subsection{Przygotowanie struktury szybkiego dostępu do informacji o instancjach należących do relacji zadanej instancji z instancjami zadanego typu}

Na tym etapie tworzona jest struktura $ ITNMPack $ dającej szybki dostęp do wszystkich sąsiadów danego typu dla określonej instancji. Wykorzystuje ona drugą właściwości zbioru uporządkowanego w Rozdziale~\ref{subsec:pair_ordering}.

Procedura tworzenia tej struktury przebiega bardzo podobnie do procedury tworzenia struktury $ ITMPack $ - główna różnica tkwi w odmiennej implementacji operatora równości dla par instancji $ FeatureInstanceTuple $, przedstawionej w Algorytmie~\ref{alg:FeatureInstanceTuple_operatorITNMPack}. Ustanawia on pary instancji jako równe sobie tylko w przypadku, gdy ich pierwsze elementy są równe względem identyfikatora oraz typu (jest to ta sama instancja), a drugie elementy są tego samego typu.

\begin{figure}[H]
\begin{algorithm}[H]
\SetKwInOut{Input}{Wejście}
\SetKwInOut{Output}{Wyjście}
\SetKwProg{myproc}{Procedure}{}{}
\SetKwFunction{proc}{operator==}
\Input{ $ a, b$ - strony operatora typu \textit{FeatureInstanceTuple}}
\Output{ $ out $ - wynik porównania }
\myproc{\proc{a, b}}{
$ out \leftarrow (field_{a_{0}} = field_{b_{0}}) \cup (featureId_{a_{1}} = featureId_{b_{1}})$\;
}
\caption{Operator równości dla par instancji typu \textit{FeatureInstanceTuple}}
\label{alg:FeatureInstanceTuple_operatorITNMPack}
\end{algorithm}
\end{figure}

Wynikiem działania funkcji \textit{reduce\_by\_key} z operatorem \ref{alg:FeatureInstanceTuple_operatorITNMPack} są trzy tablice o jednakowym rozmiarze $ c_{r} $:
\begin{itemize}
\item \textit{Uniques} - zawiera wszystkie występujące unikalne (z poziomu operatora \ref{alg:FeatureInstanceTuple_operatorITNMPack}) relacje,
\item \textit{Counts} - zawiera liczności instancji unikalnych relacji,
\item \textit{Begins} - zawiera indeksy tabeli instancji relacji, oznaczające początki obszarów ciągłych odpowiadającym elementom z tablicy \textit{Uniques}.
\end{itemize}

Dla każdego elementu tabeli \textit{Uniques} tworzony jest klucz składający się z instancji pierwszego elementu oraz typu drugiego. Następnie odpowiadającymi sobie pozycją wartościami z \textit{Counts} oraz \textit{Begins} wypełniana jest struktura \textit{NeighboursListInfoHolder} i umieszczana w $ ITNMPack.map $ przy użyciu wcześniej utworzonego klucza. 

\subsection{Wyodrębnienie ze zbioru kandydatów kolokacji maksymalnych kandydatów spełniających kryterium prewalentności}

W ostatnim kroku algorytmu zbiór kandydatów na kolokacje zostaje przefiltrowany pod względem progu powszechności. Ilość iteracji niezbędnych do zakończenia tego etapu nie jest dokładnie znana - można ją jedynie oszacować w oparciu o następujący wzór:

\begin{equation}
n_{li} \in {[ |uniqueCandidates|;\ maxLen_{cand}\ -\ 1 ]}
\end{equation}

Problem z oszacowaniem ilości iteracji wynika z faktu, że wartość $ maxLen_{cand} $, oznaczająca długość najdłuższych kandydatów, może ulec zmianie w wyniku rozbicia kandydatów na kolokacje na mniejsze w przypadku, kiedy próg powszechności jest niewystarczający do uzyskania akceptowalnego wyniku. Może to spowodować zwiększenie ilości potrzebnych iteracji, a w konsekwencji - wydłużenie czasu działania algorytmu.

Każda iteracja rozpoczyna się od pobrania ze zbioru \textit{uniqueCandidates} zbioru kandydatów \textit{toProcess} o długości $ n_{cur} $ - długość ta w pierwszej iteracji jest równa wartości $ maxLen_{cand} $. Następnie ze zbioru $ toProcess $ usuwane są duplikaty oraz kandydaci będący częścią kandydatów uznanych już za prewalentnych (należących do zbioru wynikowego). 
Wyznaczeni do przetwarzania kandydaci są umieszczani w przestrzeni pamięci GPU, co umożliwia rozpoczęcie budowy drzewa instancji, w którym każdy \textit{i}-ty poziom drzewa odpowiada instancjom o typie na \textit{i}-tej pozycji odpowiedniego kandydata. Drzewo to jest budowane dla wielu kandydatów ze zbioru $ toProcess $ jednocześnie.

Struktura drzewa jest realizowana poprzez tablice $ group_{i} $ (gdzie \textit{i} to poziom drzewa),   które przechowują informacje o indeksie powiązanego węzła na poprzednim poziomie. Jej wielkość jest równa ilości węzłów na danym poziomie. Odmiennie zachowują się tablice $ group_{i} $ dla poziomu zerowego oraz pierwszego - wskazują one na indeks kandydata, do którego należy wychodzące z poziomu zerowego poddrzewo. Dodatkowo wartości w $ group_{i} $ dla poziomu drugiego wskazują nie tylko na element nadrzędny na poziomie pierwszym, lecz także na element nadrzędny na poziomie zerowym - właściwość ta wynika z samego sposobu budowy drzewa. Pierwsze dwa poziomy drzewa budowane są w jednym kroku, ze względu na fakt, iż w poprzednich krokach algorytmu wyznaczone zostały wszystkie istniejące, spełniające kryteria relacji, pary instancji. Łatwy dostęp do tych instancji zapewnia struktura \textit{ITMPack}. 

Tworzenie poziomów drzewa można podzielić na dwa kroki. Na początku obliczana jest ilość par instancji, a następnie przy użyciu jednego wątku na znalezioną parę ilość ta wpisywana jest na daną pozycję w drzewie. Przy dodawaniu do drzewa dwóch pierwszych poziomów nie jest sprawdzana integralność dodawanych połączeń (tj. czy dodawane instancje są w relacji ze wszystkimi poprzednikami), ponieważ wynika ona bezpośrednio z relacji pomiędzy instancjami.

Na potrzeby budowania kolejnych poziomów utrzymywana jest tablica \textit{mask}, której wielkość jest nie mniejsza niż ilość instancji na poziomie $ i - 1 $. Bezpośrednio po utworzeniu zawiera wyłącznie wartości typu $ True $, co oznacza, iż wszystkie instancje na poziomie $ i - 1 $ przeszły test integralności.

Począwszy od poziomu drugiego drzewo budowane jest poprzez dodawanie do każdej instancji na $ i - 1 $ poziomie następników będących jej sąsiadami o typie wyznaczonym przez \textit{i}-tą pozycję na odpowiadającym mu kandydacie. Wykorzystując strukturę \textit{ITNMPack} (analogicznie jak przy budowie poziomu zerowego oraz pierwszego), na początku zliczana jest liczba sąsiadów spełniających określone przed chwilą kryteria - zliczanie jednak dotyczy tylko i wyłącznie instancji, dla których odpowiadające pole w tablicy \textit{mask} ma wartość $ True $, co oznacza, że dodana w poprzedniej iteracji instancja jest w relacji ze wszystkimi jej poprzednikami. Na podstawie tej wielkości kolejny poziom jest alokowany, a następnie przepisane zostają do niego instancje.

Dodane instancje niekoniecznie muszą znajdować się w relacji ze wszystkimi poprzednikami - w związku z tym niezbędne jest dodatkowe uruchomienie testu integralności dla dodanych instancji. Następuje zatem uruchomienie wątków w ilości równej liczbie nowych instancji, które to przechodzą drzewo od liścia do korzenia sprawdzając na każdym poziomie, czy nowo dodana instancja jest w relacji z instancją na obecnym poziomie. Do przechodzenia w dół użyta jest informacja zawarta o~indeksie poprzednika w \textit{group}, a wynik testu integralności zapisywany jest do tablicy \textit{mask}.

Drzewo budowane jest do momentu, w którym zostaną wyznaczone wszystkie jego poziomy (jeden na element kandydata) lub gdy na pewnym poziomie okaże się, iż nie można już dodać żadnej instancji. Po stworzeniu drzewa przy jego użyciu dokonana zostaje materializacja wszystkich instancji kandydatów. Uruchamiane jest w tym celu tyle wątków, ile wartości $ True $ w tablicy \textit{mask} odpowiadającym instancjom na ostatnim poziomie. 

Następnie można przeprowadzić proces obliczania powszechności zbioru instancji dla każdego z kandydatów w wygenerowanym zbiorze - osobno dla każdego typu składowego. Dokonuje się tego poprzez wyznaczenie unikalnych elementów w tym zbiorze i podzielenie tej wartości przez całkowitą ilość instancji tego typu. Gdy posiadane są wartości dla wszystkich typów składowych, za powszechność uznawana jest wartość minimalna.

Po obliczeniu powszechności dla wszystkich kandydatów, dla każdego z nich podejmowana jest decyzja o dodaniu do zbioru wynikowego lub rozbiciu na kandydatów o długości jeden mniejszej - każdy z nowych kandydatów powstaje przez usunięcie jednej ze składowych starego kandydata. Decyzja ta zależy od ustalonego na starcie algorytmu minimalnego progu powszechności. Nowo powstali kandydaci umieszczani są w zbiorze \textit{uniqueCandidates}, a kandydaci uznani za spełniających próg powszechności - do zbioru wynikowego.

\newpage

\section{Testy efektywnościowe}
\label{sec:tests}

Rozdział ten w całości poświęcony jest omówieniu przebiegu testów, a także prezentacji i podsumowania wyników testów wydajnościowych opracowanych implementacji algorytmu \textit{SGCT}. 

\subsection{Informacje wstępne}

Wydajność poszczególnych implementacji została przetestowana pod kątem zmienności następujących czynników: całkowitego rozmiaru danych wejściowych, progu maksymalnej odległości między punktami, do którego zachodzi relacja sąsiedztwa oraz progu minimalnej powszechności.

\subsubsection{Dane wejściowe}

Dane wygenerowano przy pomocy generatora opracowanego przez P. Boińskiego na potrzeby pracy \cite{boinski}, napisanego w języku \textit{Java}. Próbki testowe składają się z~punktów, dla których określone są trzy wartości: współrzędna $ x $, współrzędna $ y $ oraz etykieta cechy, jaką reprezentuje dany punkt. Punkty zostały dodatkowo rozmieszczone na przestrzeni $ 1000 \times 1000$. 

Dla celów pomiarowych związanych ze zmiennością rozmiaru danych wejściowych utworzone zostało osiem zestawów danych, w których liczba punktów wynosi od ok. 3000 do 200000 punktów. W przypadku pozostałych testów posłużono się czterema instancjami, które można opisać jako:

\begin{itemize}
\item \textit{dane o niskim zagęszczeniu i równomiernym rozkładzie} - około 30000 punktów rozmieszczonych losowo
\item \textit{dane o wysokim zagęszczeniu i równomiernym rozkładzie} - około 65000 punktów rozmieszczonych losowo
\item \textit{dane o niskim zagęszczeniu ze skupiskami} - około 30000 punktów z dwoma skupiskami (obszarami, na których zagęszczenie punktów jest wyższe niż poza nimi)
\item \textit{dane o wysokim zagęszczeniu ze skupiskami} - około 80000 punktów z dwoma skupiskami.
\end{itemize}

\subsubsection{Sprzęt}

Poniższe testy przeprowadzane były na następującej konfiguracji sprzętowej. 

\begin{itemize}
\item CPU - \textit{Intel Core i7-930} o częstotliwości taktowania \textit{2.80 GHz}
\item Pamięć operacyjna - \textit{24 GB RAM DDR3}
\item GPU - dwie karty graficzne \textit{nVidia GeForce GTX Titan} połączone w systemie \textit{SLI}
\item System operacyjny - \textit{Windows Server 2012 R2}
\end{itemize}

\subsubsection{Przebieg testów}

Dla uśrednienia czasów wykonań poszczególnych testów i wyeliminowania zakłóceń każdy test został powtórzony dwudziestokrotnie. Wartości i zakresy parametrów algorytmu, dla których wykonano poszczególne grupy testów, przestawia Tabela \ref{tab:params}.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Zmienny parametr} & \textbf{Próg min. powszechności} & \textbf{Próg maks. odległości} \\
\hline
Rozmiar instancji & 0.2 & 3 \\
Próg min. powszechności & 0.05 - 0.4 & 5 \\
Próg maks. odległości & 5 & 3 - 8 \\
\hline
\end{tabular}
\caption{Wartości parametrów dla poszczególnych grup testów}
\label{tab:params}
\end{table}

Testy zostały zautomatyzowane i uruchomione za pomocą skryptu napisanego w~języku \textit{PowerShell}.

\subsection{Wyniki}

\subsubsection{Zmienny rozmiar danych}

Poniższy wykres przedstawia czasy wykonań poszczególnych implementacji algorytmu w zależności od ilości punktów (Rysunek \ref{chart:datasize}).

\begin{figure}[H]
\begin{tikzpicture}
\begin{axis}[
            legend style={at={(0.5,-0.25)},anchor=north,legend cell align=left},
            xlabel={Rozmiar danych},
            ylabel={Czas (ms)},
            ymin=0, ymax=75000,
           ymajorgrids=true,
           grid style=dashed,
           width=0.95\textwidth,
           height=8cm,
           point meta={y*100}
           ]
\addplot[
          color=red,
          ultra thick,
          mark=*,
          smooth
          ]
          coordinates {
          (1701,31.9)(3624,149.15)(7526,405.75)(15289,634)(32655,2909.5)(66866,5946.55)(145686,15295.55)(204708,74280.85)
         };
        
\addplot[
          color=blue,
          ultra thick,
          mark=*,
          smooth
          ]
          coordinates {
         (1701,33.15)(3624,158.5)(7526,380.9)(15289,394.65)(32655,948.55)(66866,3255.6)(145686,6032.25)(204708,27712.3)
         };
        
\addplot[
          color=green,
          ultra thick,
          mark=*,
          smooth
          ]
          coordinates {
           (1701,1268.6)(3624,1267.6)(7526,3125.75)(15289,6147.45)(32655,7439.5)(66866,12204.6)(145686,8255.75)(204708,25806.1)
         };
             
\legend{Sequential, Parallel, GPU}
\end{axis}
\end{tikzpicture}
\caption{Wykres zależności czasu wykonania różnych implementacji algorytmu od rozmiaru danych wejściowych}
\label{chart:datasize}
\end{figure}

Z analizy wykresu wynika, że powyżej pewnego rozmiaru danych wyjściowych następuje gwałtowny wzrost czasu wykonania algorytmu. Ciekawostką jest fakt, że dla małych rozmiarów danych implementacja na procesorach kart graficznych wypada zdecydowanie gorzej niż wariant sekwencyjny dla CPU. Wynika to z kosztu przygotowania danych i konieczności inicjalizacji obliczeń równoległych, które przy dużych rozmiarach danych zostają zrekompensowane przez szybsze obliczenia, co również można łatwo zauważyć.

Przeprowadzono również oddzielne testy dla części algorytmu odpowiedzialnej za odkrywanie relacji sąsiedztwa - ich efekt prezentuje Rysunek~\ref{chart:planesweep-datasize}. Na jego podstawie można zauważyć, że do pewnego rozmiaru danych wejściowych czas wykonania wersji równoległej na CPU oraz GPU jest wyższy niż czas wykonania wersji sekwencyjnej na CPU. Wynika to z faktu, że nakład pracy utworzony przez działania mające na celu zrównoleglenie obliczeń jest na tyle duży, że w ogólnym rozrachunku szybsze jest sekwencyjne wykonanie wszystkich obliczeń niż próba ich równoleglenia. Mimo tego czas odkrywania relacji sąsiedztwa w porównaniu do ogólnego czasu wykonania algorytmu jest na tyle niski, że nie ma to większego wpływu na ogólny obraz. W~dodatku głównym zadaniem algorytmów eksploracyjnych jest przeszukiwanie dużych zestawów danych, więc sytuacja ta, która zachodzi wyłącznie przy bardzo małych ilościach danych jest dopuszczalna.

\begin{figure}[H]
\begin{tikzpicture}
\begin{axis}[
            legend style={at={(0.5,-0.25)},anchor=north,legend cell align=left},
            xlabel={Rozmiar danych},
            ylabel={Czas (ms)},
            ymin=0, ymax=45,
           ymajorgrids=true,
           grid style=dashed,
           width=0.95\textwidth,
           height=8cm,
           point meta={y*100}
           ]
\addplot[
          color=red,
          ultra thick,
          mark=*,
          smooth
          ]
          coordinates {
          (1701,2.05)(3624,6.2)(7526,18)(15289,40.9)
         };
        
\addplot[
          color=blue,
          ultra thick,
          mark=*,
          smooth
          ]
          coordinates {
         (1701,4.3)(3624,6.35)(7526,14.1)(15289,19.05)
         };
         
\addplot[
          color=green,
          ultra thick,
          dashed,
          mark=*,
          smooth
          ]
          coordinates {
         (1701,7.25)(3624,8.4)(7526,10)(15289,16)
         };
             
\legend{Sequential, Parallel, GPU}
\end{axis}
\end{tikzpicture}
\caption{Wykres zależności czasu znalezienia relacji sąsiedztwa w różnych implementacjach od rozmiaru danych wejściowych}
\label{chart:planesweep-datasize}
\end{figure}

\subsubsection{Zmienny próg maksymalnej odległości}

Wykresy zawarte w tym podrozdziale opisują zmienność czasu działania algorytmu w poszczególnych implementacjach ze względu na zmienny próg maksymalnej odległości.

\begin{figure}[H]
\begin{tikzpicture}
\begin{axis}[
            legend style={at={(0.5,-0.25)},anchor=north,legend cell align=left},
            xlabel={Próg maksymalnej odległości},
            ylabel={Czas (ms)},
            xtick=data,
            ymin=0, ymax=20000,
           ymajorgrids=true,
           grid style=dashed,
           width=0.95\textwidth,
           height=8cm,
           point meta={y*100}
           ]
\addplot[
          color=red,
          ultra thick,
          mark=*,
          smooth
          ]
          coordinates {
          (2,496.15)(2.5,1016.1)(3,3041.8)(3.5,6970.6)(4,11418.5)(4.5,18406.15)(5,11027.85)(5.5,4195.15)(6,5182.95)
         };
        
\addplot[
          color=blue,
          ultra thick,
          mark=*,
          smooth
          ]
          coordinates {
         (2,156.6)(2.5,281.35)(3,875.1)(3.5,1827.2)(4,3029.9)(4.5,5127.6)(5,3588.45)(5.5,2059.45)(6,2473.85)
         };
         
\addplot[
          color=green,
          ultra thick,
          mark=*,
          smooth
          ]
          coordinates {
         (2,5541.65)(2.5,5576.7)(3,12677.55)(3.5,19142.4)(4,16959.9)(4.5,13742.15)(5,5945)(5.5,3341.3)(6,3281.9)
         };
             
\legend{Sequential, Parallel, GPU}
\end{axis}
\end{tikzpicture}
\caption{Wykres zależności czasu wykonania implementacji algorytmu od progu maksymalnej odległości dla danych rzadkich}
\label{chart:maxdist-sparse}
\end{figure}

\begin{figure}[H]
\begin{tikzpicture}
\begin{axis}[
            legend style={at={(0.5,-0.25)},anchor=north,legend cell align=left},
            xlabel={Próg maksymalnej odległości},
            ylabel={Czas (ms)},
            xtick=data,
            ymin=0, ymax=135000,
           ymajorgrids=true,
           grid style=dashed,
           width=0.95\textwidth,
           height=8cm,
           point meta={y*100}
           ]
\addplot[
          color=red,
          ultra thick,
          mark=*,
          smooth,
          tension=0.21
          ]
          coordinates {
          (2,1926.2)(2.5,4471.5)(3,21437.6)(3.5,44259.7)(4,81484.45)(4.5,131340.8)(5,38768)(5.5,63127.05)(6,2012318)
         };
        
\addplot[
          color=blue,
          ultra thick,
          mark=*,
          smooth,
          tension=0.3
          ]
          coordinates {
         (2,529.05)(2.5,1170)(3,4561.8)(3.5,9276.9)(4,18455.7)(4.5,31357.05)(5,19851.95)(5.5,36743.55)(6,807206.6)
         };
         
\addplot[
          color=green,
          ultra thick,
          mark=*,
          smooth
          ]
          coordinates {
         (2,11745.15)(2.5,13393.2)(3,13242.15)(3.5,16658.65)(4,22906.6)(4.5,20463.1)(5,5230)(5.5,8605.05)(6,109210.4)
         };
             
\legend{Sequential, Parallel, GPU}
\end{axis}
\end{tikzpicture}
\caption{Wykres zależności czasu wykonania implementacji algorytmu od progu maksymalnej odległości dla danych gęstych}
\label{chart:maxdist-nonsparse}
\end{figure}

Na powyższych wykresach można zaobserwować wzrost czasu wykonania do pewnej wartości progu maksymalnej odległości po przekroczeniu której czasy maleć, aby po chwili znowu wrócić do tendencji wzrostowej. Wartość ta jest zbliżona dla obu implementacji na CPU, a dla GPU jest ona mniejsza.

Dla rzadkiego zbioru danych (Rysunek~\ref{chart:maxdist-sparse}) implementacja GPU jest generalnie wolniejsza od implementacji na CPU (szczególnie od implementacji równoległej), co można zauważyć głównie dla niższych wartości progu maksymalnej odległości. W~przypadku gęstego zbioru danych (Rysunek~\ref{chart:maxdist-nonsparse}) implementacja na GPU sprawuje się lepiej - od pewnego punktu prześciga oba warianty przygotowane pod CPU.

\begin{figure}[H]
\begin{tikzpicture}
\begin{axis}[
            legend style={at={(0.5,-0.25)},anchor=north,legend cell align=left},
            xlabel={Próg maksymalnej odległości},
            ylabel={Czas (ms)},
            xtick=data,
            ymin=0, ymax=4100,
           ymajorgrids=true,
           grid style=dashed,
           width=0.95\textwidth,
           height=8cm,
           point meta={y*100}
           ]
\addplot[
          color=red,
          ultra thick,
          mark=*,
          smooth
          ]
          coordinates {
          (2,65.95)(2.5,130.35)(3,221.1)(3.5,386.55)(4,490.8)(4.5,505.3)(5,483.1)(5.5,548.5)(6,704.1)
         };
        
\addplot[
          color=blue,
          ultra thick,
          mark=*,
          smooth
          ]
          coordinates {
         (2,32.95)(2.5,62.2)(3,95.15)(3.5,151.05)(4,182.2)(4.5,205.3)(5,217.35)(5.5,244.65)(6,489.35)
         };
         
\addplot[
          color=green,
          ultra thick,
          mark=*,
          smooth
          ]
          coordinates {
         (2,1527.4)(2.5,1639.75)(3,1926.4)(3.5,2353.5)(4,2532.4)(4.5,2181.65)(5,1937.9)(5.5,2025.85)(6,4024.85)
         };
             
\legend{Sequential, Parallel, GPU}
\end{axis}
\end{tikzpicture}
\caption{Wykres zależności czasu wykonania implementacji algorytmu od progu maksymalnej odległości dla danych rzadkich ze skupiskami}
\label{chart:maxdist-sparse-hotspots}
\end{figure}


\begin{figure}[H]
\begin{tikzpicture}
\begin{axis}[
            legend style={at={(0.5,-0.25)},anchor=north,legend cell align=left},
            xlabel={Próg maksymalnej odległości},
            ylabel={Czas (ms)},
            xtick=data,
            ymin=0, ymax=500000,
           ymajorgrids=true,
           grid style=dashed,
           width=0.95\textwidth,
           height=8cm,
           point meta={y*100}
           ]
\addplot[
          color=red,
          ultra thick,
          mark=*,
          smooth
          ]
          coordinates {
          (2,2707.25)(2.5,10790.95)(3,30445.7)(3.5,54134.4)(4,65445.65)(4.5,23831.25)(5,410738.6)
         };
        
\addplot[
          color=blue,
          ultra thick,
          mark=*,
          smooth
          ]
          coordinates {
         (2,817.3)(2.5,2596.55)(3,6764.9)(3.5,12306.4)(4,21553.95)(4.5,11650.45)(5,140731.4)
         };
         
\addplot[
          color=green,
          ultra thick,
          mark=*,
          smooth
          ]
          coordinates {
         (2,14687.95)(2.5,25544.8)(3,20642.55)(3.5,24312.45)(4,15273.4)(4.5,10056.6)(5,35529.55)
         };
             
\legend{Sequential, Parallel, GPU}
\end{axis}
\end{tikzpicture}
\caption{Wykres zależności czasu wykonania implementacji algorytmu od progu maksymalnej odległości dla danych gęstych ze skupiskami}
\label{chart:maxdist-nonsparse-hotspots}
\end{figure}

Podobne testy zostały wykonane dla danych, w których występują tzw. skupiska. Dla rzadkich danych, których część jest zgrupowana w skupiska (Rysunek~\ref{chart:maxdist-sparse-hotspots}) implementacja na GPU wypada nadzwyczaj słabo w porównaniu do implementacji na CPU. W przypadku danych gęstych (Rysunek~\ref{chart:maxdist-nonsparse-hotspots}) 
wydajność poszczególnych implementacji jest analogiczna jak dla danych gęstych w których nie występują skupiska. Wynika to z faktu, że przy dużym zagęszczeniu danych skupienie ich części w pewnych sektorach nie powoduje znacznego zmniejszenia gęstości poza nimi, więc ogólny rozkład danych w dalszym ciągu jest dość równomierny.

\subsubsection{Zmienny próg minimalnej powszechności}

Ostatni test skupia się na porównaniu czasów działania poszczególnych implementacji ze względu na zmienny próg minimalnej powszechności. Efekty tego porównania przedstawia Rysunek~\ref{chart:minprev-sparse}.

\begin{figure}[H]
\begin{tikzpicture}
\begin{axis}[
            legend style={at={(0.5,-0.25)},anchor=north,legend cell align=left},
            xlabel={Próg minimalnej powszechności},
            ylabel={Czas (ms)},
            xtick=data,
            ymin=0, ymax=35000,
           ymajorgrids=true,
           grid style=dashed,
           width=0.95\textwidth,
           height=8cm,
           point meta={y*100}
           ]
\addplot[
          color=red,
          ultra thick,
          mark=*,
          smooth,
          tension=0.2
          ]
          coordinates {
          (0.1,14785.55)(0.15,3050)(0.2,11616.7)(0.25,31008.85)(0.3,1237.55)(0.35,1202.5)(0.4,4862.3)(0.45,419.4)(0.5,414.35)
         };
        
\addplot[
          color=blue,
          ultra thick,
          mark=*,
          smooth
          ]
          coordinates {
         (0.1,6558.35)(0.15,1496.4)(0.2,3678.6)(0.25,8034.6)(0.3,1063.35)(0.35,1081.9)(0.4,1579.35)(0.45,316.05)(0.5,289)
         };
         
\addplot[
          color=green,
          ultra thick,
          mark=*,
          smooth
          ]
          coordinates {
         (0.1,12485)(0.15,3156.85)(0.2,6082.4)(0.25,17783.9)(0.3,2247.6)(0.35,2229.7)(0.4,5129.7)(0.45,2189.7)(0.5,2276.85)
         };
             
\legend{Sequential, Parallel, GPU}
\end{axis}
\end{tikzpicture}
\caption{Wykres zależności czasu wykonania implementacji algorytmu od progu minimalnej powszechności}
\label{chart:minprev-sparse}
\end{figure}

Zmienność progu minimalnej powszechności nie ma większego wpływu na zależności pomiędzy czasami wykonań poszczególnych implementacji - implementacja GPU pozostaje wolniejsza niż implementacja równoległa na CPU oraz szybsza niż implementacja sekwencyjna. Zmienia się to jedynie w przypadku wysokich progów powszechności (powyżej 0.3) - implementacja GPU jest wtedy wolniejsza również od implementacji sekwencyjnej. Wynika to z faktu, że przy takim progu minimalnej powszechności odfiltrowana zostaje większość danych wejściowych - w takim przypadku implementacje dla CPU pozostają bez pracy, natomiast wersja dla procesora GPU w dalszym ciągu musi wykonać wszelkie działania związane z kopiowaniem danych do pamięci karty graficznej oraz niezbędnymi przygotowaniami do równoległego wykonania algorytmu.

\newpage

\section{Zakończenie}
\label{sec:fin}

W niniejszej pracy poruszono problem odkrywania kolokacji przestrzennych, czyli poszukiwania typów obiektów często występujących blisko siebie. Z pewnością tematyka ta należy do jednych z ciekawszych zagadnień, jeżeli chodzi o eksplorację danych - tym bardziej, że w dobie Internetu oraz swobodnego dzielenia się informacjami dostęp do niezbędnych danych jest wyjątkowo łatwy. Również moc obliczeniowa nie stanowi problemu w dzisiejszych czasach - dzięki współczesnym, skutecznym algorytmom proces odkrywania danych można bez obaw przeprowadzać na współczesnych komputerach osobistych.

Do takich algorytmów z pewnością należy także algorytm \textit{SGCT} \cite{chinczyki} będący głównym tematem niniejszej pracy. Stanowi on godną alternatywę dla klasycznych metod odkrywania kolokacji przestrzennych, pozwalając na efektywne wyszukiwanie kolokacji w systemach z ograniczoną pamięcią. Dzięki zaproponowanym usprawnieniom możliwe było jego zaimplementowanie zarówno w środowisku sekwencyjnym, jak i równoległym - również w wersji na procesory graficzne.

W tym celu została wykorzystana popularna technologia \textit{CUDA} firmy \textit{NVIDIA}. Z pewnością opanowanie tej technologii w stopniu umożliwiającym efektywną realizację algorytmu \textit{SGCT} na procesory graficzne wymagało wielkiego wysiłku - tym bardziej, że w porównaniu do implementacji dla procesorów CPU środowisko \textit{CUDA} nie oferuje wielu rozszerzeń ułatwiających pracę nad tworzeniem wielowątkowych aplikacji i wiele rzeczy dostępnych "od ręki" na CPU wymagało oddzielnego opracowania specjalnie pod wariant GPU.

Na podstawie przeprowadzonych testów można zaryzykować stwierdzenie, że wysiłek ten był opłacalny. Dla większych danych o dużej gęstości implementacja dla GPU znacząco zyskuje w stosunku do wariantu równoległego dla CPU opracowanego w oparciu o bibliotekę \textit{Parallel Pattern Library} firmy Microsoft. Przy mniejszych danych różnice nie są jednak takie oczywiste.
Całkowitą porażką okazała się sekwencyjny algorytm opracowany dla procesora CPU, który niemal w każdym przypadku wykazuje się niższą efektywnością niż pozostałe implementacje. Dobitnie to pokazuje, jakie efekty można uzyskać przy zastosowaniu modelu równoległego w procesie implementacji algorytmów - nie tylko tych dotyczących eksploracji danych.

W trakcie pracy udało się zauważyć możliwe dalsze kierunku rozwoju opracowania zaprezentowanych w niniejszej pracy implementacji w celu uzyskania większej wydajności.

.

.

.

.

.

Powyższe rozważania nie mają jednak wpływu na końcowy wniosek płynący z tej pracy: obliczenia na kartach graficznych są przyszłością działu eksploracji danych, a także jakichkolwiek algorytmów wymagających sporej mocy obliczeniowej. Oczywiście stanie się tak tylko pod warunkiem, że powstające algorytmy będą w stanie wykorzystać potencjał drzemiący w równoległości, co często nie jest proste i wymaga dodatkowej pracy nad przemyśleniem działania algorytmu pod kątem potencjalnego zrównoleglenia. Niewątpliwie jednak końcowe efekty są tego warte.
 
\newpage

\begin{thebibliography}{}
% Autorzy, Tytuł, Gdzie ?, Rok wydania, Strony
\bibitem{cuda_dp}Adinetz A., CUDA Dynamic Parallelism API and Principles (NVIDIA Developer Blogs), 20-04-2014 [dostęp: 10-01-2017]. Dostępny w Internecie: https://devblogs.nvidia.com/parallelforall/cuda-dynamic-parallelism-api-principles
\bibitem{asoc}Agrawal R., Srikant R.. Fast Algorithms for Mining Association Rules in Large Databases. W: Proceedings of the 20th International Conference on Very Large Data Bases (VLDB 1994), USA, 1994, strony 487–499.
\bibitem{spatial}Arge L., Procopiuc O., Ramaswamy S., Suel T., Vitter J. Scalable Sweeping- Based Spatial Join. W: Proceedings of the International Conference on Very Large Databases (VLDB 1998), USA, 1998.
\bibitem{kurwa}Bellman R. E. Adaptive Control Processes - A Guided Tour, USA, 1961.
\bibitem{boinski}Boiński P., Przetwarzanie zbiorów przestrzennych zapytań eksploracyjnych w środowiskach z ograniczonym rozmiarem pamięci operacyjnej, Poznań, 2015.
\bibitem{kerbosz}Bron C., Kerbosch J. Algorithm 457: Finding all cliques of an undirected graph. W: Communications of the ACM nr 16, USA, 1973, strony 575–577.
\bibitem{pivot2}Cazals F., Karande C. A note on the problem of reporting maximal cliques. W: Theoretical Computer Science nr 407, USA, 2008, strony 564–568.
\bibitem{professional_cuda}Cheng J., Grossman M., McKerche T. Professional CUDA C Programming, Wrox, 2014.
\bibitem{9sec2}Clementini E., Di Felice P., van Oosterom P. A small set of formal topological relationships suitable for end-user interaction. W: Proceedings of the 3rd International Symposium on Advances in Spatial Databases (SSD 1993), Wielka Brytania, 1993, strony 277-295.
\bibitem{9sec}Egenhofer M. J., Franzosa R. Point-set topological spatial relations. W: International Journal of Geographic Information Systems nr 5, 1991, strony 161–174.
\bibitem{degenerat}Eppstein D., Löffler M., Strash D. Listing all maximal cliques in sparsegraphs in near-optimal time. W: 21st International Symposium On Algorithms And Computation, Niemcy, 2010, strony 403–414.
\bibitem{toptrendy}Ester M., Frommelt A., Kriegel H. P., Sander J. Algorithms for characterization and trend detection in spatial databases. W: Proceedings of the 4th International Conference on Knowledge Discovery and Data Mining (KDD 1998), USA, 1998, strony 44–50.
\bibitem{klasyfikacja}Ester M., Kriegel H. P., Sander J. Spatial Data Mining: A Database Approach. W: In Proceedings of the 5th International Symposium on Advances in Spatial Databases (SSD 1997), Wielka Brytania, 1997, strony 47–66.
\bibitem{kdd}Fayyad U., Piatetsky-Shapiro G., Smyth P. From Data Mining to Knowledge Discovery in Databases. W: AI Magazine nr 17, 1996, strony 37-54.
\bibitem{cuda_um}Harris M., Unified Memory in CUDA 6 (NVIDIA Developer Blogs), 18-11-2013 [dostęp: 10-01-2017]. Dostępny w Internecie: https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6
\bibitem{fieldmodel}Jones C., Hall M. A Field Based Representation for Vague Areas Defined by Spatial Prepositions. W: Proceedings of the Workshop on Methodologies and Resources for Processing Spatial Language at 6th Language Resources and Evaluation Conference (LREC 2008), Maroko, 2008.
\bibitem{asoc2}Koperski K., Han J. Discovery of Spatial Association Rules in Geographic Information Databases. W: Proceedings of the 4th International Symposium on Advances in Spatial Databases (SSD 1995) nr 951, USA, 1995, strony 47–66.
\bibitem{matusiak}Matula D. W., Beck L. L. Smallest-last ordering and clustering and graph coloring algorithms. W: Journal of the ACM, nr 30, USA, 1983, strony 417–427.
\bibitem{klasykchuj1}Miller H. J., Han J. Geographic Data Mining and Knowledge Discovery, USA, 2001.
\bibitem{trad}Miller H. J., Han J. Geographic Data Mining and Knowledge Discovery.Bristol, USA, 2001.
\bibitem{classsets}Morimoto Y. Mining Frequent Neighboring Class Sets in Spatial Databases. W: Proceedings of the 7th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2001), USA, 2001, strony 353–358.
\bibitem{mikromiekki}Praca zbiorowa, Comparing the Concurrency Runtime to Other Concurrency Models (Microsoft), 2015 [dostęp: 10-01-2017]. Dostępny w Internecie: https://msdn.microsoft.com/en-us/library/dd998048.aspx\#openmp
\bibitem{stak}Praca zbiorowa, Microsoft Parallel Patterns Library (PPL) vs. OpenMP (Stack Overflow), 14-03-2012 [dostęp: 10-01-2017]. Dostępny w Internecie: http://stackoverflow.com/questions/9700088/microsoft-parallel-patterns-library-ppl-vs-openmp
\bibitem{id3}Quinlan J. R. Induction of Decision Trees. W: Machine Learning nr 1, USA, 1986, strony 81–106.
\bibitem{klasykchuj2}Roddick J. F., Spiliopoulou M. A Bibliography of Temporal, Spatial and Spatio-Temporal data Mining Research. W: ACM SIGKDD Exploration Newsletter nr 1, USA, 1999, strony 34–38.
\bibitem{cuda_by_examples}Sanders J., Kandrot E., CUDA by Example: An Introduction to General-Purpose GPU Programming, Addison-Wesley, 2011.
\bibitem{przypadeg}Shekhar S., Chawla S. Spatial Databases: A Tour, USA, 2003.
\bibitem{huang}Shekhar S., Huang Y. Discovering Spatial Co-location Patterns: A Summary of Results. W: SSTD, 2001, strony 236–256.
\bibitem{multihuang}Shekhar S., Huang Y. The Multi-resolution Co-location Miner: A New Algorithm to Find Co-location Patterns in Spatial Dataset. W: Technical Report 02-019, USA, 2002.
\bibitem{pivot}Tomita E., Tanaka A., Takahashi H. The worst-case time complexity for generating all maximal cliques and computational experiments. W: Theoretical Computer Science nr 363, USA, 2006, strony 28–42.
\bibitem{buffered-sort}Vinodsu, Sorting in PPL, Parallel Programming in Native Code (Microsoft Developer Network Blogs), 14-01-2011 [dostęp: 10-01-2017]. Dostępny w Internecie: https://blogs.msdn.microsoft.com/nativeconcurrency/2011/01/14/sorting-in-ppl/
\bibitem{icpi}Wang L., Bao Y., Lu J. Efficient Discovery of Spatial Co-Location Patterns Using the iCPI-tree. W: The Open Information Systems Journal nr 3, 2009, strony 69–80.
\bibitem{cpi}Wang L., Bao Y., Lu J., Yip J. A New Join-less Approach for Co-location Pattern Mining. W: Proceedings of the 8th IEEE International Conference on Computer and Information Technology (CIT 2008), Sydney, 2008, strony 197–202.
\bibitem{wang}Wang L., Zhou L., Lu J., Yip, J. (2009). An order-clique-based approach for mining maximal co-locations. W: Information Sciences nr 179, USA, 2009, strony 3370–3382.
\bibitem{chinczyki}Yao X., Peng L., Yang L., Chi T. A fast space-saving algorithm for maximal co-location pattern mining. W: Expert Systems with Applications nr 63, 2016, strony 310–323.
\bibitem{joinless}Yoo J. S., Shekhar S. A Joinless Approach for Mining Spatial Colocation Patterns. W: IEEE Transactions on Knowledge and Data Engineering, USA, 2006.
\end{thebibliography}

\end{document}
